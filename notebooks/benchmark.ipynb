{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "218861dd-58a9-4792-b848-bc433eeafe68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU: Tesla P40\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current device index (default is 0 if no other device is specified)\n",
    "    current_device = torch.cuda.current_device()\n",
    "    \n",
    "    # Get the name of the GPU at this device index\n",
    "    gpu_name = torch.cuda.get_device_name(current_device)\n",
    "    print(f\"Current GPU: {gpu_name}\")\n",
    "else:\n",
    "    print(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbcdfe67-1cb9-45be-86e6-2cceaaed871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fde9a592-c977-428d-854b-723edd495e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer\n",
    "from bert_score import score\n",
    "\n",
    "# Calculate BERT\n",
    "def calcuate_bert(reference:str, candidate:str):\n",
    "    P, R, F1 = score([candidate], [reference], lang=\"ja\")  # Set language to Japanese\n",
    "    #print(f\"BERTScore: Precision={P.mean():.4f}, Recall={R.mean():.4f}, F1={F1.mean():.4f}\")\n",
    "    return {\n",
    "        'precision': float(P),\n",
    "        'recall': float(R),\n",
    "        'f1_score': float(F1)\n",
    "    }\n",
    "    \n",
    "\n",
    "# Calculate ROUGE, ROUGE-L\n",
    "def calculate_rouge(reference, generated, n=1, model_id = \"CohereForAI/aya-23-8B\"):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # Tokenize the input strings into words\n",
    "    reference_tokens = tokenizer.tokenize(reference) #reference.split()\n",
    "    generated_tokens = tokenizer.tokenize(generated) #generated.split()\n",
    "    \n",
    "    # Generate n-grams\n",
    "    reference_ngrams = list(ngrams(reference_tokens, n))\n",
    "    generated_ngrams = list(ngrams(generated_tokens, n))\n",
    "    \n",
    "    # Count n-grams\n",
    "    reference_count = Counter(reference_ngrams)\n",
    "    generated_count = Counter(generated_ngrams)\n",
    "\n",
    "    # Calculate matched n-grams\n",
    "    matched_ngrams = reference_count & generated_count\n",
    "    \n",
    "    # Precision\n",
    "    precision = (sum(matched_ngrams.values()) / len(generated_ngrams)) if generated_ngrams else 0.0\n",
    "    \n",
    "    # Recall\n",
    "    recall = (sum(matched_ngrams.values()) / len(reference_ngrams)) if reference_ngrams else 0.0\n",
    "    \n",
    "    # F1 Score\n",
    "    if precision + recall > 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1_score = 0.0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score\n",
    "    }\n",
    "\n",
    "def lcs_length(x, y):\n",
    "    \"\"\"Calculate the length of the longest common subsequence (LCS)\"\"\"\n",
    "    m, n = len(x), len(y)\n",
    "    # Create a 2D array to store lengths of longest common subsequence.\n",
    "    lcs_table = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    # Fill the lcs_table\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if x[i - 1] == y[j - 1]:\n",
    "                lcs_table[i][j] = lcs_table[i - 1][j - 1] + 1\n",
    "            else:\n",
    "                lcs_table[i][j] = max(lcs_table[i - 1][j], lcs_table[i][j - 1])\n",
    "\n",
    "    return lcs_table[m][n]\n",
    "\n",
    "def calculate_rouge_l(reference, generated, model_id = \"CohereForAI/aya-23-8B\"):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # Tokenize the input strings into words\n",
    "    reference_tokens = tokenizer.tokenize(reference) #reference.split()\n",
    "    generated_tokens = tokenizer.tokenize(generated) #generated.split()\n",
    "\n",
    "    # Calculate the length of the longest common subsequence\n",
    "    lcs_len = lcs_length(reference_tokens, generated_tokens)\n",
    "\n",
    "    # Precision\n",
    "    precision = lcs_len / len(generated_tokens) if generated_tokens else 0.0\n",
    "\n",
    "    # Recall\n",
    "    recall = lcs_len / len(reference_tokens) if reference_tokens else 0.0\n",
    "\n",
    "    # F1 Score\n",
    "    if precision + recall > 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1_score = 0.0\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score\n",
    "    }\n",
    "\n",
    "# Calculate BLEU-score\n",
    "def brevity_penalty(candidate, reference):\n",
    "    \"\"\"\n",
    "    Calculates the brevity penalty given the candidate and reference sentences.\n",
    "    \"\"\"\n",
    "    reference_length = len(reference)\n",
    "    candidate_length = len(candidate)\n",
    "\n",
    "    if reference_length < candidate_length:\n",
    "        BP = 1\n",
    "    else:\n",
    "        penalty = 1 - (reference_length / candidate_length)\n",
    "        BP = np.exp(penalty)\n",
    "\n",
    "    return BP\n",
    "\n",
    "\n",
    "def average_clipped_precision(candidate:str, reference:str,n:int):\n",
    "    \"\"\"\n",
    "    Calculates the precision given the candidate and reference sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    clipped_precision_score = []\n",
    "    \n",
    "    # Loop through values 1, 2, 3, 4. This is the length of n-grams\n",
    "    for n_gram_length in range(1, n):\n",
    "        reference_n_gram_counts = Counter(ngrams(reference, n_gram_length))        \n",
    "        candidate_n_gram_counts = Counter(ngrams(candidate, n_gram_length))\n",
    "\n",
    "        total_candidate_ngrams = sum(candidate_n_gram_counts.values())       \n",
    "        \n",
    "        for ngram in candidate_n_gram_counts: \n",
    "            # check if it is in the reference n-gram\n",
    "            if ngram in reference_n_gram_counts:\n",
    "                # if the count of the candidate n-gram is bigger than the corresponding\n",
    "                # count in the reference n-gram, then set the count of the candidate n-gram \n",
    "                # to be equal to the reference n-gram\n",
    "                \n",
    "                if candidate_n_gram_counts[ngram] > reference_n_gram_counts[ngram]: \n",
    "                    candidate_n_gram_counts[ngram] = reference_n_gram_counts[ngram] # t\n",
    "                                                   \n",
    "            else:\n",
    "                candidate_n_gram_counts[ngram] = 0 # else set the candidate n-gram equal to zero\n",
    "\n",
    "        clipped_candidate_ngrams = sum(candidate_n_gram_counts.values())\n",
    "        \n",
    "        clipped_precision_score.append(clipped_candidate_ngrams / total_candidate_ngrams)\n",
    "    \n",
    "    # Calculate the geometric average: take the mean of elemntwise log, then exponentiate\n",
    "    # This is equivalent to taking the n-th root of the product as shown in equation (1) above\n",
    "    s = np.exp(np.mean(np.log(clipped_precision_score)))\n",
    "    \n",
    "    return s\n",
    "\n",
    "def calculate_bleu_score(reference:str,candidate:str, n:int):\n",
    "    assert n >=2, \"n must >= 2\"\n",
    "    BP = brevity_penalty(candidate, reference)    \n",
    "    geometric_average_precision = average_clipped_precision(candidate, reference, n)    \n",
    "    return BP * geometric_average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5fd0efd-e06a-46ff-83c6-d3d2035dfcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Splits: dict_keys(['train'])\n"
     ]
    }
   ],
   "source": [
    "# !huggingface-cli download dataset longquan/llm-japanese-dataset-split_10\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"longquan/llm-japanese-dataset-split_10\", cache_dir=\"~/.cache/huggingface/datasets\")\n",
    "\n",
    "# View available dataset splits\n",
    "print(\"Available Splits:\", dataset.keys())\n",
    "\n",
    "# Load specific split (e.g., 'train') and inspect the first few rows\n",
    "train_data = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5263a8b7-dc23-4e6b-8454-a5e381e05155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from random import seed, sample\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9143d07c-cdd1-4714-be44-7335ba2e3106",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d9c475d-ebab-41dd-a9de-c97e7de22f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference:  長谷川 きよし（はせがわ きよし、1949年7月13日 - ）は、日本のシンガーソングライター、ギタリスト。本名は長谷川 清志。東京都出身。 \n",
      "\n",
      "Model:  長谷川きよしは、1978年生まれの日本のシンガポール・ポップ・バンド。バンド名の「きよし」は、長谷川きよしが本名の「きよしが」に由来する。\n",
      "\n",
      "バンドのメンバーは、長谷川きよし、加代麻子、滝本修司、平田隆司、加藤正人、加藤麻美、平田敏明、平田隆司、平田敏明、平田隆司、平田敏明、平田隆司\n"
     ]
    }
   ],
   "source": [
    "random_sample = train_data[random.choice(range(train_data.num_rows))]\n",
    "\n",
    "reference = random_sample['output']\n",
    "print(\"Reference: \",reference,\"\\n\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": random_sample[\"instruction\"]},\n",
    "    {\"role\": \"user\", \"content\": random_sample[\"input\"]}\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=128,\n",
    ")\n",
    "candidate = outputs[0][\"generated_text\"][-1]['content']\n",
    "print(\"Model: \", candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8fb699d-c0da-4c89-9947-e1c5628a9e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loc/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT: {'precision': 0.6437352895736694, 'recall': 0.679200291633606, 'f1_score': 0.660992443561554}\n",
      "BLEU-2: 0.2535211267605634\n",
      "BLEU-4: 0.13222487680372713\n",
      "ROUGE-2: {'precision': 0.039603960396039604, 'recall': 0.0975609756097561, 'f1_score': 0.056338028169014086}\n",
      "ROUGE-3: {'precision': 0.01, 'recall': 0.025, 'f1_score': 0.014285714285714285}\n",
      "ROUGE-L: {'precision': 0.10784313725490197, 'recall': 0.2619047619047619, 'f1_score': 0.1527777777777778}\n"
     ]
    }
   ],
   "source": [
    "print(\"BERT:\", calcuate_bert(reference, candidate))\n",
    "print(\"BLEU-2:\", calculate_bleu_score(reference, candidate,2))\n",
    "print(\"BLEU-4:\", calculate_bleu_score(reference, candidate,4))\n",
    "print(\"ROUGE-2:\", calculate_rouge(reference, candidate, n=2))\n",
    "print(\"ROUGE-3:\", calculate_rouge(reference, candidate, n=3))\n",
    "rouge_l = calculate_rouge_l(reference, candidate)\n",
    "print(\"ROUGE-L:\", rouge_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e80020c-2d98-4ae1-8f21-ccb45842c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(reference:str, candidate:str):\n",
    "    bert_score = calcuate_bert(reference, candidate)\n",
    "    bleu_2_score = calculate_bleu_score(reference, candidate,2)\n",
    "    bleu_4_score = calculate_bleu_score(reference, candidate,4)\n",
    "    rouge_2_score = calculate_rouge(reference, candidate, n=2)\n",
    "    calculate_rouge_l(reference, candidate)\n",
    "    return bert_score, bleu_2_score, bleu_4_score, rouge_2_score, rouge_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cedcd275-220b-4190-900f-18b71433940f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT: {'precision': 0.6437352895736694, 'recall': 0.679200291633606, 'f1_score': 0.660992443561554}\n",
      "BLEU-2: 0.2535211267605634\n",
      "BLEU-4: 0.13222487680372713\n",
      "ROUGE-2: {'precision': 0.039603960396039604, 'recall': 0.0975609756097561, 'f1_score': 0.056338028169014086}\n",
      "ROUGE-L: {'precision': 0.10784313725490197, 'recall': 0.2619047619047619, 'f1_score': 0.1527777777777778}\n"
     ]
    }
   ],
   "source": [
    "bert_score, bleu_2_score, bleu_4_score, rouge_2_score, rouge_l = calculate_metrics(reference, candidate)\n",
    "\n",
    "print(\"BERT:\", bert_score)\n",
    "print(\"BLEU-2:\", bleu_2_score)\n",
    "print(\"BLEU-4:\", bleu_4_score)\n",
    "print(\"ROUGE-2:\", rouge_2_score)\n",
    "print(\"ROUGE-L:\", rouge_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "809fc337-4d20-4212-b750-6d051c01d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(42)  # Set seed for reproducibility\n",
    "random_indices = sample(range(train_data.num_rows), 10)  # Randomly choose 10 indices\n",
    "subset = train_data.select(random_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d04d9ed0-e84c-4715-b0a0-a4d1444dfa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/tmp/ipykernel_11176/2832792305.py:151: RuntimeWarning: divide by zero encountered in log\n",
      "  s = np.exp(np.mean(np.log(clipped_precision_score)))\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metrics for Subset:\n",
      "BERT: 0.6702380239963531\n",
      "BLEU-2: 0.2746584187704523\n",
      "BLEU-4: 0.16628310886013314\n",
      "ROUGE-2: 0.14740961635365704\n",
      "ROUGE-L: 0.15277777777777776\n",
      "Consuming time:  44.97154903411865\n"
     ]
    }
   ],
   "source": [
    "# Start counting time\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize accumulators for each metric\n",
    "bert_total, bleu_2_total, bleu_4_total, rouge_2_total, rouge_l_total = 0, 0, 0, 0, 0\n",
    "\n",
    "# Loop through the subset\n",
    "for i in range(10):\n",
    "    \n",
    "    # Current sample \n",
    "    current_sample = subset[i]\n",
    "    \n",
    "    reference = current_sample['output']\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": current_sample[\"instruction\"]},\n",
    "        {\"role\": \"user\", \"content\": current_sample[\"input\"]}\n",
    "    ]\n",
    "    outputs = pipe(\n",
    "        messages,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "    candidate = outputs[0][\"generated_text\"][-1]['content']\n",
    "    \n",
    "    # Calculate metrics for the current pair\n",
    "    bert_score, bleu_2_score, bleu_4_score, rouge_2_score, rouge_l = calculate_metrics(reference, candidate)\n",
    "    \n",
    "    # Accumulate scores\n",
    "    bert_total += bert_score[\"f1_score\"]\n",
    "    bleu_2_total += bleu_2_score\n",
    "    bleu_4_total += bleu_4_score\n",
    "    rouge_2_total += rouge_2_score[\"f1_score\"]\n",
    "    rouge_l_total += rouge_l[\"f1_score\"]\n",
    "\n",
    "# Compute average scores for the subset\n",
    "num_items = len(subset)\n",
    "bert_avg = bert_total / num_items\n",
    "bleu_2_avg = bleu_2_total / num_items\n",
    "bleu_4_avg = bleu_4_total / num_items\n",
    "rouge_2_avg = rouge_2_total / num_items\n",
    "rouge_l_avg = rouge_l_total / num_items\n",
    "\n",
    "# Print the average metrics\n",
    "print(\"Average Metrics for Subset:\")\n",
    "print(\"BERT:\", bert_avg)\n",
    "print(\"BLEU-2:\", bleu_2_avg)\n",
    "print(\"BLEU-4:\", bleu_4_avg)\n",
    "print(\"ROUGE-2:\", rouge_2_avg)\n",
    "print(\"ROUGE-L:\", rouge_l_avg)\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print(f\"Consuming time: \",end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc26b98-7467-43a5-a872-0eaf9407a92d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
