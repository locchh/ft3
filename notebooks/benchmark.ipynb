{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "218861dd-58a9-4792-b848-bc433eeafe68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU: Tesla P40\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current device index (default is 0 if no other device is specified)\n",
    "    current_device = torch.cuda.current_device()\n",
    "    \n",
    "    # Get the name of the GPU at this device index\n",
    "    gpu_name = torch.cuda.get_device_name(current_device)\n",
    "    print(f\"Current GPU: {gpu_name}\")\n",
    "else:\n",
    "    print(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbcdfe67-1cb9-45be-86e6-2cceaaed871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fde9a592-c977-428d-854b-723edd495e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer\n",
    "from bert_score import score\n",
    "\n",
    "# Calculate BERT\n",
    "def calcuate_bert(reference:str, candidate:str):\n",
    "    P, R, F1 = score([candidate], [reference], lang=\"ja\")  # Set language to Japanese\n",
    "    #print(f\"BERTScore: Precision={P.mean():.4f}, Recall={R.mean():.4f}, F1={F1.mean():.4f}\")\n",
    "    return {\n",
    "        'precision': float(P),\n",
    "        'recall': float(R),\n",
    "        'f1_score': float(F1)\n",
    "    }\n",
    "    \n",
    "\n",
    "# Calculate ROUGE, ROUGE-L\n",
    "def calculate_rouge(reference, generated, n=1, model_id = \"CohereForAI/aya-23-8B\"):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # Tokenize the input strings into words\n",
    "    reference_tokens = tokenizer.tokenize(reference) #reference.split()\n",
    "    generated_tokens = tokenizer.tokenize(generated) #generated.split()\n",
    "    \n",
    "    # Generate n-grams\n",
    "    reference_ngrams = list(ngrams(reference_tokens, n))\n",
    "    generated_ngrams = list(ngrams(generated_tokens, n))\n",
    "    \n",
    "    # Count n-grams\n",
    "    reference_count = Counter(reference_ngrams)\n",
    "    generated_count = Counter(generated_ngrams)\n",
    "\n",
    "    # Calculate matched n-grams\n",
    "    matched_ngrams = reference_count & generated_count\n",
    "    \n",
    "    # Precision\n",
    "    precision = (sum(matched_ngrams.values()) / len(generated_ngrams)) if generated_ngrams else 0.0\n",
    "    \n",
    "    # Recall\n",
    "    recall = (sum(matched_ngrams.values()) / len(reference_ngrams)) if reference_ngrams else 0.0\n",
    "    \n",
    "    # F1 Score\n",
    "    if precision + recall > 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1_score = 0.0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score\n",
    "    }\n",
    "\n",
    "def lcs_length(x, y):\n",
    "    \"\"\"Calculate the length of the longest common subsequence (LCS)\"\"\"\n",
    "    m, n = len(x), len(y)\n",
    "    # Create a 2D array to store lengths of longest common subsequence.\n",
    "    lcs_table = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    # Fill the lcs_table\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if x[i - 1] == y[j - 1]:\n",
    "                lcs_table[i][j] = lcs_table[i - 1][j - 1] + 1\n",
    "            else:\n",
    "                lcs_table[i][j] = max(lcs_table[i - 1][j], lcs_table[i][j - 1])\n",
    "\n",
    "    return lcs_table[m][n]\n",
    "\n",
    "def calculate_rouge_l(reference, generated, model_id = \"CohereForAI/aya-23-8B\"):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # Tokenize the input strings into words\n",
    "    reference_tokens = tokenizer.tokenize(reference) #reference.split()\n",
    "    generated_tokens = tokenizer.tokenize(generated) #generated.split()\n",
    "\n",
    "    # Calculate the length of the longest common subsequence\n",
    "    lcs_len = lcs_length(reference_tokens, generated_tokens)\n",
    "\n",
    "    # Precision\n",
    "    precision = lcs_len / len(generated_tokens) if generated_tokens else 0.0\n",
    "\n",
    "    # Recall\n",
    "    recall = lcs_len / len(reference_tokens) if reference_tokens else 0.0\n",
    "\n",
    "    # F1 Score\n",
    "    if precision + recall > 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1_score = 0.0\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score\n",
    "    }\n",
    "\n",
    "# Calculate BLEU-score\n",
    "def brevity_penalty(candidate, reference):\n",
    "    \"\"\"\n",
    "    Calculates the brevity penalty given the candidate and reference sentences.\n",
    "    \"\"\"\n",
    "    reference_length = len(reference)\n",
    "    candidate_length = len(candidate)\n",
    "\n",
    "    if reference_length < candidate_length:\n",
    "        BP = 1\n",
    "    else:\n",
    "        penalty = 1 - (reference_length / candidate_length)\n",
    "        BP = np.exp(penalty)\n",
    "\n",
    "    return BP\n",
    "\n",
    "\n",
    "def average_clipped_precision(candidate:str, reference:str,n:int):\n",
    "    \"\"\"\n",
    "    Calculates the precision given the candidate and reference sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    clipped_precision_score = []\n",
    "    \n",
    "    # Loop through values 1, 2, 3, 4. This is the length of n-grams\n",
    "    for n_gram_length in range(1, n):\n",
    "        reference_n_gram_counts = Counter(ngrams(reference, n_gram_length))        \n",
    "        candidate_n_gram_counts = Counter(ngrams(candidate, n_gram_length))\n",
    "\n",
    "        total_candidate_ngrams = sum(candidate_n_gram_counts.values())       \n",
    "        \n",
    "        for ngram in candidate_n_gram_counts: \n",
    "            # check if it is in the reference n-gram\n",
    "            if ngram in reference_n_gram_counts:\n",
    "                # if the count of the candidate n-gram is bigger than the corresponding\n",
    "                # count in the reference n-gram, then set the count of the candidate n-gram \n",
    "                # to be equal to the reference n-gram\n",
    "                \n",
    "                if candidate_n_gram_counts[ngram] > reference_n_gram_counts[ngram]: \n",
    "                    candidate_n_gram_counts[ngram] = reference_n_gram_counts[ngram] # t\n",
    "                                                   \n",
    "            else:\n",
    "                candidate_n_gram_counts[ngram] = 0 # else set the candidate n-gram equal to zero\n",
    "\n",
    "        clipped_candidate_ngrams = sum(candidate_n_gram_counts.values())\n",
    "        \n",
    "        clipped_precision_score.append(clipped_candidate_ngrams / total_candidate_ngrams)\n",
    "    \n",
    "    # Calculate the geometric average: take the mean of elemntwise log, then exponentiate\n",
    "    # This is equivalent to taking the n-th root of the product as shown in equation (1) above\n",
    "    s = np.exp(np.mean(np.log(clipped_precision_score)))\n",
    "    \n",
    "    return s\n",
    "\n",
    "def calculate_bleu_score(reference:str,candidate:str, n:int):\n",
    "    assert n >=2, \"n must >= 2\"\n",
    "    BP = brevity_penalty(candidate, reference)    \n",
    "    geometric_average_precision = average_clipped_precision(candidate, reference, n)    \n",
    "    return BP * geometric_average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5fd0efd-e06a-46ff-83c6-d3d2035dfcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Splits: dict_keys(['train'])\n"
     ]
    }
   ],
   "source": [
    "# !huggingface-cli download dataset longquan/llm-japanese-dataset-split_10\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"longquan/llm-japanese-dataset-split_10\", cache_dir=\"~/.cache/huggingface/datasets\")\n",
    "\n",
    "# View available dataset splits\n",
    "print(\"Available Splits:\", dataset.keys())\n",
    "\n",
    "# Load specific split (e.g., 'train') and inspect the first few rows\n",
    "train_data = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5263a8b7-dc23-4e6b-8454-a5e381e05155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9143d07c-cdd1-4714-be44-7335ba2e3106",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d9c475d-ebab-41dd-a9de-c97e7de22f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference:  番組はＮＨＫラジオセンタースタッフの岩崎博（ミスター・イワサキ）が進行役を務め、特に１９６０年代・１９７０年代のイージーリスニングをメインとした洋楽のナンバーを多数かけ、それを交えつつ、岩崎の一人コント的なトーク（季節や時事ネタ）を楽しむ。 \n",
      "\n",
      "Model:  番組はNHKラジオセンターの岩崎博（ミスターイワサキ）が進行を務め、特に1960年代と1970年代のエジプト音楽を中心とした洋楽のナンバーを多く取り上げ、岩崎の一人コント的なトーク（季節や時事ネタ）を楽しむ。\n"
     ]
    }
   ],
   "source": [
    "random_sample = train_data[random.choice(range(train_data.num_rows))]\n",
    "\n",
    "reference = random_sample['output']\n",
    "print(\"Reference: \",reference,\"\\n\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": random_sample[\"instruction\"]},\n",
    "    {\"role\": \"user\", \"content\": random_sample[\"input\"]}\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=128,\n",
    ")\n",
    "candidate = outputs[0][\"generated_text\"][-1]['content']\n",
    "print(\"Model: \", candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8fb699d-c0da-4c89-9947-e1c5628a9e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loc/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT: {'precision': 0.8950253129005432, 'recall': 0.890752911567688, 'f1_score': 0.8928839564323425}\n",
      "BLEU-2: 0.6595414762772626\n",
      "BLEU-4: 0.5759595650977029\n",
      "ROUGE-2: {'precision': 0.5873015873015873, 'recall': 0.44047619047619047, 'f1_score': 0.5034013605442177}\n",
      "ROUGE-3: {'precision': 0.4838709677419355, 'recall': 0.3614457831325301, 'f1_score': 0.41379310344827586}\n",
      "ROUGE-L: {'precision': 0.703125, 'recall': 0.5294117647058824, 'f1_score': 0.6040268456375839}\n"
     ]
    }
   ],
   "source": [
    "print(\"BERT:\", calcuate_bert(reference, candidate))\n",
    "print(\"BLEU-2:\", calculate_bleu_score(reference, candidate,2))\n",
    "print(\"BLEU-4:\", calculate_bleu_score(reference, candidate,4))\n",
    "print(\"ROUGE-2:\", calculate_rouge(reference, candidate, n=2))\n",
    "print(\"ROUGE-3:\", calculate_rouge(reference, candidate, n=3))\n",
    "rouge_l = calculate_rouge_l(reference, candidate)\n",
    "print(\"ROUGE-L:\", rouge_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e80020c-2d98-4ae1-8f21-ccb45842c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(reference:str, candidate:str):\n",
    "    bert_score = calcuate_bert(reference, candidate)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
