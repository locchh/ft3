{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db2fc400-6956-46dc-9de2-ece157eea885",
   "metadata": {},
   "source": [
    "### Dataset: locchuong/llama_conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9418c7cf-37e1-49be-b18b-4e55a578d47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU: Tesla P40\n",
      "Import Successfull!\n",
      "Available Splits: dict_keys(['train'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Created!\n",
      "Reference:  重罪謀殺化法則は、一部の英米法諸法域において適用されます。近代的な解釈では、当該重罪が本質的に危険なものであるか、または行為者が明らかに危険な方法において行ったことが求められます。この法則は、危険な重罪の抑止手段として正当化されることもあります。ただし、イングランドおよびウェールズと北アイルランドでは廃止されており、一部の法域では同様の規定によって代替されています。 \n",
      "\n",
      "Model:  重罪謀殺化法則は、重罪の過程中で行為者が適用ある重罪の過程において偶発的にまたは具体的な殺意なく人を死に至らしめた場合、謀殺となるというものです。\n",
      "\n",
      "この法則は、次のような方法で使われることがあります。\n",
      "\n",
      "1.  **刑事責任の拡大**: 重罪の過程中で行為者が行った行為が、死に至るまでの過程で行われた行為と見なされる場合、刑事責任を拡大します。たとえば、殺意なく死に至るまでの過程で、行為者がその過程で死に至った場合、刑事責任は拡大されます。\n",
      "2.  **死の原因を定義する**: 重罪の過程中で行為者が死に至るまでの過程で死に至った場合、死の原因を定義する必要があります。たとえば、死に至るまでの過程で、行為者がその過程で死に至った場合、死の原因を定義する必要\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loc/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT: 0.6745099425315857\n",
      "BLEU-2: 0.3063063063063063\n",
      "BLEU-4: 0.16848802831087525\n",
      "ROUGE-2: 0.1099476439790576\n",
      "ROUGE-L: 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import random\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "import torch\n",
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current device index (default is 0 if no other device is specified)\n",
    "    current_device = torch.cuda.current_device()\n",
    "    \n",
    "    # Get the name of the GPU at this device index\n",
    "    gpu_name = torch.cuda.get_device_name(current_device)\n",
    "    print(f\"Current GPU: {gpu_name}\")\n",
    "else:\n",
    "    print(\"No GPU available.\")\n",
    "\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from utils.helper import calculate_metrics\n",
    "\n",
    "print(\"Import Successfull!\")\n",
    "\n",
    "# Load dataset\n",
    "dataset_name = \"locchuong/llama_conversations\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# View available dataset splits\n",
    "print(\"Available Splits:\", dataset.keys())\n",
    "\n",
    "# Load specific split (e.g., 'train') and inspect the first few rows\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "# Create pipeline\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"Pipeline Created!\")\n",
    "\n",
    "# Select random sample\n",
    "random_sample = train_data[random.choice(range(train_data.num_rows))]\n",
    "tag = random_sample[\"tag\"]\n",
    "messages = random_sample[\"conversations\"]\n",
    "if tag == \"aixsatoshi/cosmopedia-japanese-100k\":\n",
    "    reference = messages[-1]['content']\n",
    "    max_new_tokens = 512\n",
    "else:\n",
    "    reference = messages[-1]['content']\n",
    "    messages = messages[:-1] # Cut off assistant's response\n",
    "    max_new_tokens = 256\n",
    "\n",
    "print(\"Reference: \",reference,\"\\n\")\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=max_new_tokens,\n",
    ")\n",
    "\n",
    "candidate = outputs[0][\"generated_text\"][-1]['content']\n",
    "\n",
    "print(\"Model: \", candidate)\n",
    "\n",
    "bert_score, bleu_2_score, bleu_4_score, rouge_2_score, rouge_l = calculate_metrics(reference, candidate)\n",
    "print(\"BERT:\", bert_score)\n",
    "print(\"BLEU-2:\", bleu_2_score)\n",
    "print(\"BLEU-4:\", bleu_4_score)\n",
    "print(\"ROUGE-2:\", rouge_2_score)\n",
    "print(\"ROUGE-L:\", rouge_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff703f5-2491-4abb-8879-86c129289525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': '重罪謀殺化法則とは何ですか？', 'role': 'user'},\n",
       " {'content': '重罪謀殺化法則は、謀殺罪を2つの方向に拡大する法理で、行為者が適用ある重罪の過程において偶発的にまたは具体的な殺意なく人を死に至らしめた場合、謀殺となるというものです。また、この法則によりかかる重罪への加担者は、当該重罪の過程またはその助長の際に生じたいかなる死についても刑事責任を負います。',\n",
       "  'role': 'assistant'},\n",
       " {'content': '重罪謀殺化法則の起源はどこにありますか？', 'role': 'user'},\n",
       " {'content': '重罪謀殺化法則の概念の起源は古代ローマ法にありますが、近代的な形態を得たのは18世紀です。古典的な例としては、密猟者が鹿に向けて射た矢が藪の中に隠れていた少年に当たった場合、密猟者はその意図しない結果であっても、重罪謀殺の責任を負うとされます。',\n",
       "  'role': 'assistant'},\n",
       " {'content': '重罪謀殺化法則はどのように使われるのですか？', 'role': 'user'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e97fda-971d-449b-ba6b-65ecb1559d49",
   "metadata": {},
   "source": [
    "### Benchmark: locchuong/llama-longquan-llm-japanese-dataset-split_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a68b8361-d43a-48ed-9dd4-b52b9e1df9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU: Tesla P40\n",
      "Import Successfull!\n",
      "Available Splits: dict_keys(['train'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Created!\n",
      "Reference:  コロムビア・レコード（Columbia Records）は、アメリカのレコードレーベルの一つ。現在はソニーグループ完全子会社のソニー・ミュージックエンタテインメント米国法人（SMEI）の一部門となっている。 \n",
      "\n",
      "Model:  コロムビア・レコード（Colombia Records）とは、米国で1950年代から1970年代にかけて活躍した、ラジオ・テープ・レーベルである。コロムビア・レコードは、多くのアーティストがレーベルからレコードやシングルをリリースした。\n",
      "\n",
      "コロムビア・レコードは、1958年にラッパー・デイビッド・バラードが設立した。バラードは、1950年代後半から1960年代にかけて、コロム\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loc/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT: 0.6925212740898132\n",
      "BLEU-2: 0.328042328042328\n",
      "BLEU-4: 0.21045793553487732\n",
      "ROUGE-2: 0.1382978723404255\n",
      "ROUGE-L: 0.19999999999999998\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import random\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "import torch\n",
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current device index (default is 0 if no other device is specified)\n",
    "    current_device = torch.cuda.current_device()\n",
    "    \n",
    "    # Get the name of the GPU at this device index\n",
    "    gpu_name = torch.cuda.get_device_name(current_device)\n",
    "    print(f\"Current GPU: {gpu_name}\")\n",
    "else:\n",
    "    print(\"No GPU available.\")\n",
    "\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from utils.helper import calculate_metrics\n",
    "\n",
    "print(\"Import Successfull!\")\n",
    "\n",
    "# Load dataset\n",
    "dataset_name = \"locchuong/llama-longquan-llm-japanese-dataset-split_10\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# View available dataset splits\n",
    "print(\"Available Splits:\", dataset.keys())\n",
    "\n",
    "# Load specific split (e.g., 'train') and inspect the first few rows\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "# Create pipeline\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"Pipeline Created!\")\n",
    "\n",
    "# Select random sample\n",
    "random_sample = train_data[random.choice(range(train_data.num_rows))]\n",
    "tag = random_sample[\"tag\"]\n",
    "messages = random_sample[\"conversations\"]\n",
    "reference = messages[-1]['content']\n",
    "messages = messages[:-1]\n",
    "    \n",
    "print(\"Reference: \",reference,\"\\n\")\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=128,\n",
    ")\n",
    "\n",
    "candidate = outputs[0][\"generated_text\"][-1]['content']\n",
    "\n",
    "print(\"Model: \", candidate)\n",
    "\n",
    "bert_score, bleu_2_score, bleu_4_score, rouge_2_score, rouge_l = calculate_metrics(reference, candidate)\n",
    "print(\"BERT:\", bert_score)\n",
    "print(\"BLEU-2:\", bleu_2_score)\n",
    "print(\"BLEU-4:\", bleu_4_score)\n",
    "print(\"ROUGE-2:\", rouge_2_score)\n",
    "print(\"ROUGE-L:\", rouge_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d3c9dd-7719-4953-8dad-a8396af46e83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
