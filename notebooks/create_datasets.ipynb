{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1350b10-dccb-40e2-8aed-23a8466fd59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU: Tesla P40\n",
      "Import Successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current device index (default is 0 if no other device is specified)\n",
    "    current_device = torch.cuda.current_device()\n",
    "    \n",
    "    # Get the name of the GPU at this device index\n",
    "    gpu_name = torch.cuda.get_device_name(current_device)\n",
    "    print(f\"Current GPU: {gpu_name}\")\n",
    "else:\n",
    "    print(\"No GPU available.\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, DatasetDict, load_dataset, concatenate_datasets\n",
    "from utils.helper import convert_to_llama_format\n",
    "\n",
    "print(\"Import Successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "898e037d-318c-4888-98af-a8a2f14a3d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataset using a pandas DataFrame\n",
    "empty_df = pd.DataFrame(columns=[\"conversations\", \"tag\"])  # Define your schema here\n",
    "empty_dataset = Dataset.from_pandas(empty_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f762bcbf-1798-4134-b739-15b63243739a",
   "metadata": {},
   "source": [
    "### aixsatoshi/cosmopedia-japanese-100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1889baab-20f5-42b8-b3e2-c70b3ec148ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['format', 'japanese', 'english', 'audience', 'seed_data'],\n",
       "        num_rows: 99866\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"aixsatoshi/cosmopedia-japanese-100k\"\n",
    "dataset = load_dataset(repo_id)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ce9c041-4085-4127-b319-f0ba58e1e758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Splits: dict_keys(['train'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a translator, your job is translate English to Japanese'},\n",
       " {'role': 'user', 'content': 'Translate the given content to Japanese'},\n",
       " {'role': 'user',\n",
       "  'content': \" The world of education is vast and diverse, encompassing everything from kindergarten classrooms to doctoral programs. But when it comes to the internet, there's one category of domains that is strictly reserved for a specific type of educational institution: the .edu domain. According to the extract provided, this domain is only available to post-secondary institutions that are accredited at the institutional level by agencies recognized by the US Department of Education. This means that elementary schools, high schools, middle schools, and even charter schools don't qualify for this prestigious designation.\\n\\nBut what does it really mean to be an accredited institution? And why is the .edu domain so important? Let's dive deeper into these questions and explore some of the nuances of the topic.\\n\\nAccording to the Council for Higher Education Accreditation (CHEA), accreditation is a process of peer review that aims to ensure quality and accountability in higher education. Institutional accreditation evaluates an entire college or university, taking into account factors such as its mission, governance, academic offerings, student support services, and financial stability. By contrast, programmatic accreditation focuses on individual departments or majors within an institution, rather than the school as a whole.\\n\\nSo why is accreditation important? For students, it can provide assurance that they are attending a reputable institution that meets rigorous standards of quality. Employers also value degrees from accredited institutions, as they demonstrate that graduates have received a solid education in their field. Additionally, many forms of federal aid require attendance at an accredited institution, which means that unaccredited schools may be off limits to students seeking financial assistance.\\n\\nWithin the context of the .edu domain, accreditation serves as a gatekeeper, ensuring that only legitimate post-secondary institutions are able to secure this highly coveted online real estate. While anyone can technically create a website using any domain name, having a .edu address signals to visitors that they are dealing with a bona fide educational organization. This can help build trust and credibility, both of which are essential in today's digital age.\\n\\nOf course, not all accredited institutions are created equal. Some may offer more rigorous academics, better resources, and stronger career outcomes than others. As the extract notes, it's important to do your research and understand the differences between various certifications and degree programs. Simply being accredited doesn't necessarily make a school the best fit for every student.\\n\\nOne example of a reputable accredited institution is the International Association of Certified Home Inspectors (InterNACHI). With over 25,000 members worldwide, InterNACHI offers training, certification, and other resources for professional home inspectors. To become an InterNACHI Certified Home Inspector, individuals must meet certain requirements, including passing a background check, completing a series of courses and exams, and adhering to strict ethical guidelines. Once certified, inspectors can use the prestigious Certified Professional Inspector CPI® credential, signifying their expertise and commitment to excellence.\\n\\nIn addition to its certification program, InterNACHI operates a separate entity called the InterNACHI School, which offers a variety of certificate programs and continuing education courses for inspectors. Notably, the school recently secured the .edu domain mentioned in the extract, further bolstering its reputation as a leader in the field. Students who complete the Home Inspector Certificate Program through the InterNACHI School will receive official transcripts bearing the .edu domain, providing tangible proof of their education and credentials.\\n\\nIn conclusion, the .edu domain is an exclusive club, reserved for a select group of accredited post-secondary institutions.\"},\n",
       " {'role': 'assistant',\n",
       "  'content': '  教育の世界は広く多様で、幼稚園の教室から博士課程プログラムに至るまで、あらゆるものを含んでいます。しかし、インターネットの世界では、特定の教育機関にのみ厳密に割り当てられているカテゴリーが 1 つあります。それが .edu ドメインです。抽出文によると、このドメインは、米国教育省が機関レベルで認定した高等教育機関のみに厳密に予約されています。つまり、幼稚園、高校、中学校、さらにはチャーター スクールでさえ、この威信のある指定を受ける資格がありません。\\n\\nでは、認定とは何を意味するのでしょうか。なぜ .edu ドメインがそれほど重要なのでしょうか。これらの質問について深く掘り下げ、いくつかの詳細を見ていきましょう。\\n\\n高等教育認定評議会 (CHEA) によると、認定は、質と説明責任を確保するためにピア レビューのプロセスを使用することを目的としています。機関認定は、機関全体を評価し、その使命、ガバナンス、学問的提供、学生支援サービス、および財政的安定性などの要素を考慮します。一方、プログラム認定は、機関内の個々の学部や専攻を対象としており、機関全体ではなく、個別の部門または専攻に焦点を当てています。\\n\\nでは、なぜ認定が重要なのでしょうか。学生にとっては、質の高い教育機関に属し、厳格な基準を満たしていることが保証されます。雇用主も、認定された機関の卒業生には、自分の分野での質の高い教育を受けていることを信頼しています。さらに、多くの連邦援助は認定された機関への出席を必要とするため、認定されていない学校では、学生は財政援助を求めることができない場合があります。\\n\\n.edu ドメインの文脈において、認定は門番のようなものであり、最高級のオンライン不動産を確保できるのは、厳格な審査に合格した後にのみ適格な高等教育機関のみです。誰でも技術的にはどのドメイン名でもウェブサイトを作成できますが、.edu アドレスを持つことで、訪問者に教育機関と取引していることを伝えることができます。これにより、信頼性と信用性を築き、今日のデジタル時代において不可欠なものになります。\\n\\nもちろん、すべての認定機関が同等に優れているわけではありません。ある機関は、より厳しい学業、より優れたリソース、より強いキャリア成果を提供するところもあれば、他の機関よりも劣るところもあります。抽出された文で述べられているように、自分の研究を行い、認定と学位プログラムの違いを理解することが重要です。認定されているからといって、すべての学生にとって最高の機関であるとは限りません。\\n\\n一例として、国際認定住宅検査官協会 (InterNACHI) は立派な認定機関です。世界中に 25,000 人以上の会員を擁する InterNACHI は、住宅検査官向けのトレーニング、認定、その他のリソースを提供しています。 InterNACHI 認定ホームインスペクターになるには、厳格な審査、一連のコースと試験の完了、厳格な倫理規定への準拠など、いくつかの要件を満たす必要があります。認定されると、インスペクターは厳格な CPI 認定プロフェッショナル インスペクター CPI® 資格を使用できます。これにより、専門性と卓越性への取り組みが証明され、専門家としての地位が確立されます。\\n\\nさらに、InterNACHI は、別の組織である InterNACHI スクールを運営しており、住宅検査官向けにさまざまな認定プログラムと継続教育コースを提供しています。最近、InterNACHI スクールは、前述の .edu ドメインを確保し、その権威ある機関としての地位をさらに確固たるものにしました。 InterNACHI 住宅検査員認定プログラムを修了した InterNACHI スクールの学生は、認定証を受け取ることができます。この証明書には、公式のトランスクリプトが記載されており、教育と資格を目に見える形で証明することができます。\\n\\n結論として、.edu ドメインは、厳格な認定を受けた高等教育機関のみに厳密に予約されている排他的なクラブです。\\n'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View available dataset splits\n",
    "print(\"Available Splits:\", dataset.keys())\n",
    "\n",
    "# Load specific split (e.g., 'train') and inspect the first few rows\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "sample = train_data[random.choice(range(train_data.num_rows))] #train_data[0]\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a translator, your job is translate English to Japanese\"},\n",
    "    {\"role\": \"user\", \"content\" : \"Translate the given content to Japanese\"},\n",
    "    {\"role\": \"user\", \"content\" : sample['english']},\n",
    "    {\"role\": \"assistant\", \"content\" : sample['japanese']},\n",
    "     ]\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a293a40c-1d44-4b5d-b8bf-52a5dfb3c17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 99866/99866 [00:02<00:00, 34178.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop through the source dataset and append data to the empty dataset\n",
    "new_data = {\"conversations\": [], \"tag\": []}  # Initialize dictionary to store new data\n",
    "for row in tqdm(train_data):\n",
    "    # Create messages\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a translator, your job is translate English to Japanese\"},\n",
    "    {\"role\": \"user\", \"content\" : \"Translate the given content to Japanese\"},\n",
    "    {\"role\": \"user\", \"content\" : sample['english']},\n",
    "    {\"role\": \"assistant\", \"content\" : sample['japanese']},\n",
    "     ]\n",
    "    \n",
    "    new_data[\"conversations\"].append(messages)\n",
    "    new_data[\"tag\"].append(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84609664-a353-45c1-907c-fa5f2601288f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['conversations', 'tag'],\n",
      "    num_rows: 99866\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Add the collected data to the empty dataset\n",
    "updated_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "# Concatenate them\n",
    "empty_dataset = concatenate_datasets([empty_dataset, updated_dataset])\n",
    "\n",
    "# Print the resulting dataset\n",
    "print(empty_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b5e9d0-5b62-4a33-8957-6383814b4f76",
   "metadata": {},
   "source": [
    "### shi3z/Japanese_wikipedia_conversation_100K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23980b64-2bb5-42b9-b8df-45183abfda02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 110322\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"shi3z/Japanese_wikipedia_conversation_100K\"\n",
    "dataset = load_dataset(repo_id)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7b573fa-1814-4e4f-8ba4-8b16a4ab4828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Splits: dict_keys(['train'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'オルデンドルフはどんな場所ですか？'},\n",
       " {'role': 'assistant', 'content': 'オルデンドルフは都市や町の名前ではありません。おそらく誤字です。'},\n",
       " {'role': 'user', 'content': 'オルデンドルフとは何のことですか？'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'オルデンドルフは人名の一つです。詳細な情報がないため、具体的な人物を特定することはできません。'},\n",
       " {'role': 'user', 'content': 'オルデンドルフってどんな名前ですか？'},\n",
       " {'role': 'assistant', 'content': 'オルデンドルフは、ドイツ語の姓の一例です。一般的な名前ではありません。'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View available dataset splits\n",
    "print(\"Available Splits:\", dataset.keys())\n",
    "\n",
    "# Load specific split (e.g., 'train') and inspect the first few rows\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "sample = train_data[random.choice(range(train_data.num_rows))] #train_data[0]\n",
    "\n",
    "messages = convert_to_llama_format(sample['conversations'])\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "645e5f5b-1b8e-465a-8d6e-c0ae32d68ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 110322/110322 [00:04<00:00, 26711.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop through the source dataset and append data to the empty dataset\n",
    "new_data = {\"conversations\": [], \"tag\": []}  # Initialize dictionary to store new data\n",
    "for row in tqdm(train_data):\n",
    "    # Create messages\n",
    "    messages = convert_to_llama_format(row['conversations'])    \n",
    "    new_data[\"conversations\"].append(messages)\n",
    "    new_data[\"tag\"].append(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57578f48-5317-4e27-b014-e093e634bd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['conversations', 'tag'],\n",
      "    num_rows: 210188\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Add the collected data to the empty dataset\n",
    "updated_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "# Concatenate them\n",
    "empty_dataset = concatenate_datasets([empty_dataset, updated_dataset])\n",
    "\n",
    "# Print the resulting dataset\n",
    "print(empty_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9974aec-8a42-4b19-a591-3fb3dab73aff",
   "metadata": {},
   "source": [
    "### FreedomIntelligence/alpaca-gpt4-japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04d7b143-a7aa-4443-be4c-b041df3bcf52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations', 'id'],\n",
       "        num_rows: 49969\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"FreedomIntelligence/alpaca-gpt4-japanese\"\n",
    "dataset = load_dataset(repo_id)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a7567cf-a760-4663-9e87-39d4769b28af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Splits: dict_keys(['train'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': '次の文を前置詞句を追加して、より詳細な文に編集してください。\\n私はオフィスに入りました。'},\n",
       " {'role': 'assistant', 'content': '私はガラスのドアを通ってオフィスに入りました。'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View available dataset splits\n",
    "print(\"Available Splits:\", dataset.keys())\n",
    "\n",
    "# Load specific split (e.g., 'train') and inspect the first few rows\n",
    "train_data = dataset[\"train\"]\n",
    "sample = train_data[random.choice(range(train_data.num_rows))] #train_data[0]\n",
    "\n",
    "messages = convert_to_llama_format(sample['conversations'])\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb741269-2084-43f0-bdf9-45d39f7b2ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 49969/49969 [00:01<00:00, 39086.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop through the source dataset and append data to the empty dataset\n",
    "new_data = {\"conversations\": [], \"tag\": []}  # Initialize dictionary to store new data\n",
    "for row in tqdm(train_data):\n",
    "    # Create messages\n",
    "    messages = convert_to_llama_format(row['conversations'])    \n",
    "    new_data[\"conversations\"].append(messages)\n",
    "    new_data[\"tag\"].append(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c21f91f-ee47-439c-8fe7-10f66c9657ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['conversations', 'tag'],\n",
      "    num_rows: 260157\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Add the collected data to the empty dataset\n",
    "updated_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "# Concatenate them\n",
    "empty_dataset = concatenate_datasets([empty_dataset, updated_dataset])\n",
    "\n",
    "# Print the resulting dataset\n",
    "print(empty_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e5d7be-129a-4a43-934f-bef7edd2c333",
   "metadata": {},
   "source": [
    "### FreedomIntelligence/evol-instruct-japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec3060c6-7f03-4e51-9601-42504bc76e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations', 'id'],\n",
       "        num_rows: 59022\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"FreedomIntelligence/evol-instruct-japanese\"\n",
    "dataset = load_dataset(repo_id)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cfab4ab-d7d5-4197-9f90-b94b714a22be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Splits: dict_keys(['train'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'MATLABコードを使用して、さまざまな種類の再生可能エネルギー源の効率を分析することはできますか？調査結果の概要を提供してください。\\n以下は始めるためのサンプルコードです。\\nenergySources = [\"solar\", \"wind\", \"hydro\"];\\nefficiency = [0.18、0.3、0.9];\\nbar(efficiency)\\nxticklabels(energySources)\\nylabel(\"Efficiency\")\\ntitle(\"再生可能エネルギー源の比較\")\\n'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'はい、MATLABコードを使用して、さまざまな種類の再生可能エネルギー源の効率を分析することができます。上記のサンプルコードは、太陽光、風力、水力の三つの再生可能エネルギー源の効率を比較するためのものです。バーを使用してグラフを描画し、xticklabelsでX軸に各エネルギー源の名前を表示し、ylabelでY軸に効率を表示し、titleでグラフのタイトルを設定しています。\\n\\nこのコードを応用することで、さまざまな効率を持つ再生可能エネルギー源を比較することができます。また、データを増やすことで、より詳細な分析や比較を行うことも可能です。\\n\\n例えば、サンプルコードを以下のように変更して、太陽光、風力、水力の他に地熱およびバイオマスを追加することができます。\\n\\nenergySources = [\"solar\", \"wind\", \"hydro\", \"geothermal\", \"biomass\"];\\nefficiency = [0.18, 0.3, 0.9, 0.1, 0.35];\\nbar(efficiency)\\nxticklabels(energySources)\\nylabel(\"Efficiency\")\\ntitle(\"再生可能エネルギー源の比較\")\\n\\nまた、MATLABでは、様々な統計解析ツールや機械学習ツールを使用することもできるため、より高度な分析を行うこともできます。例えば、時間帯や天候データを組み合わせることで、各再生可能エネルギー源の最適な運用状況を分析することができます。'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View available dataset splits\n",
    "print(\"Available Splits:\", dataset.keys())\n",
    "\n",
    "# Load specific split (e.g., 'train') and inspect the first few rows\n",
    "train_data = dataset[\"train\"]\n",
    "sample = train_data[random.choice(range(train_data.num_rows))] #train_data[0]\n",
    "\n",
    "messages = convert_to_llama_format(sample['conversations'])\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1000c618-5c45-4dd6-bcf8-a8b0eb4ad6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 59022/59022 [00:02<00:00, 27802.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop through the source dataset and append data to the empty dataset\n",
    "new_data = {\"conversations\": [], \"tag\": []}  # Initialize dictionary to store new data\n",
    "for row in tqdm(train_data):\n",
    "    # Create messages\n",
    "    messages = convert_to_llama_format(row['conversations'])    \n",
    "    new_data[\"conversations\"].append(messages)\n",
    "    new_data[\"tag\"].append(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d863abb-9837-402d-8aa5-7e358cd15b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['conversations', 'tag'],\n",
      "    num_rows: 319179\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Add the collected data to the empty dataset\n",
    "updated_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "# Concatenate them\n",
    "empty_dataset = concatenate_datasets([empty_dataset, updated_dataset])\n",
    "\n",
    "# Print the resulting dataset\n",
    "print(empty_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e828bad7-0f38-4d41-b5ac-b8e08fc53242",
   "metadata": {},
   "source": [
    "Shuffle and random select sample in new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a03ee3ef-abd8-4dd7-b37f-2c482676cffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'tag'],\n",
       "    num_rows: 319179\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the dataset\n",
    "shuffled_dataset = empty_dataset.shuffle(seed=42)  # Optional seed for reproducibility\n",
    "shuffled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2834889e-5cee-4862-987d-52d5c631f5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'You are a translator, your job is translate English to Japanese',\n",
       "   'role': 'system'},\n",
       "  {'content': 'Translate the given content to Japanese', 'role': 'user'},\n",
       "  {'content': \" The world of education is vast and diverse, encompassing everything from kindergarten classrooms to doctoral programs. But when it comes to the internet, there's one category of domains that is strictly reserved for a specific type of educational institution: the .edu domain. According to the extract provided, this domain is only available to post-secondary institutions that are accredited at the institutional level by agencies recognized by the US Department of Education. This means that elementary schools, high schools, middle schools, and even charter schools don't qualify for this prestigious designation.\\n\\nBut what does it really mean to be an accredited institution? And why is the .edu domain so important? Let's dive deeper into these questions and explore some of the nuances of the topic.\\n\\nAccording to the Council for Higher Education Accreditation (CHEA), accreditation is a process of peer review that aims to ensure quality and accountability in higher education. Institutional accreditation evaluates an entire college or university, taking into account factors such as its mission, governance, academic offerings, student support services, and financial stability. By contrast, programmatic accreditation focuses on individual departments or majors within an institution, rather than the school as a whole.\\n\\nSo why is accreditation important? For students, it can provide assurance that they are attending a reputable institution that meets rigorous standards of quality. Employers also value degrees from accredited institutions, as they demonstrate that graduates have received a solid education in their field. Additionally, many forms of federal aid require attendance at an accredited institution, which means that unaccredited schools may be off limits to students seeking financial assistance.\\n\\nWithin the context of the .edu domain, accreditation serves as a gatekeeper, ensuring that only legitimate post-secondary institutions are able to secure this highly coveted online real estate. While anyone can technically create a website using any domain name, having a .edu address signals to visitors that they are dealing with a bona fide educational organization. This can help build trust and credibility, both of which are essential in today's digital age.\\n\\nOf course, not all accredited institutions are created equal. Some may offer more rigorous academics, better resources, and stronger career outcomes than others. As the extract notes, it's important to do your research and understand the differences between various certifications and degree programs. Simply being accredited doesn't necessarily make a school the best fit for every student.\\n\\nOne example of a reputable accredited institution is the International Association of Certified Home Inspectors (InterNACHI). With over 25,000 members worldwide, InterNACHI offers training, certification, and other resources for professional home inspectors. To become an InterNACHI Certified Home Inspector, individuals must meet certain requirements, including passing a background check, completing a series of courses and exams, and adhering to strict ethical guidelines. Once certified, inspectors can use the prestigious Certified Professional Inspector CPI® credential, signifying their expertise and commitment to excellence.\\n\\nIn addition to its certification program, InterNACHI operates a separate entity called the InterNACHI School, which offers a variety of certificate programs and continuing education courses for inspectors. Notably, the school recently secured the .edu domain mentioned in the extract, further bolstering its reputation as a leader in the field. Students who complete the Home Inspector Certificate Program through the InterNACHI School will receive official transcripts bearing the .edu domain, providing tangible proof of their education and credentials.\\n\\nIn conclusion, the .edu domain is an exclusive club, reserved for a select group of accredited post-secondary institutions.\",\n",
       "   'role': 'user'},\n",
       "  {'content': '  教育の世界は広く多様で、幼稚園の教室から博士課程プログラムに至るまで、あらゆるものを含んでいます。しかし、インターネットの世界では、特定の教育機関にのみ厳密に割り当てられているカテゴリーが 1 つあります。それが .edu ドメインです。抽出文によると、このドメインは、米国教育省が機関レベルで認定した高等教育機関のみに厳密に予約されています。つまり、幼稚園、高校、中学校、さらにはチャーター スクールでさえ、この威信のある指定を受ける資格がありません。\\n\\nでは、認定とは何を意味するのでしょうか。なぜ .edu ドメインがそれほど重要なのでしょうか。これらの質問について深く掘り下げ、いくつかの詳細を見ていきましょう。\\n\\n高等教育認定評議会 (CHEA) によると、認定は、質と説明責任を確保するためにピア レビューのプロセスを使用することを目的としています。機関認定は、機関全体を評価し、その使命、ガバナンス、学問的提供、学生支援サービス、および財政的安定性などの要素を考慮します。一方、プログラム認定は、機関内の個々の学部や専攻を対象としており、機関全体ではなく、個別の部門または専攻に焦点を当てています。\\n\\nでは、なぜ認定が重要なのでしょうか。学生にとっては、質の高い教育機関に属し、厳格な基準を満たしていることが保証されます。雇用主も、認定された機関の卒業生には、自分の分野での質の高い教育を受けていることを信頼しています。さらに、多くの連邦援助は認定された機関への出席を必要とするため、認定されていない学校では、学生は財政援助を求めることができない場合があります。\\n\\n.edu ドメインの文脈において、認定は門番のようなものであり、最高級のオンライン不動産を確保できるのは、厳格な審査に合格した後にのみ適格な高等教育機関のみです。誰でも技術的にはどのドメイン名でもウェブサイトを作成できますが、.edu アドレスを持つことで、訪問者に教育機関と取引していることを伝えることができます。これにより、信頼性と信用性を築き、今日のデジタル時代において不可欠なものになります。\\n\\nもちろん、すべての認定機関が同等に優れているわけではありません。ある機関は、より厳しい学業、より優れたリソース、より強いキャリア成果を提供するところもあれば、他の機関よりも劣るところもあります。抽出された文で述べられているように、自分の研究を行い、認定と学位プログラムの違いを理解することが重要です。認定されているからといって、すべての学生にとって最高の機関であるとは限りません。\\n\\n一例として、国際認定住宅検査官協会 (InterNACHI) は立派な認定機関です。世界中に 25,000 人以上の会員を擁する InterNACHI は、住宅検査官向けのトレーニング、認定、その他のリソースを提供しています。 InterNACHI 認定ホームインスペクターになるには、厳格な審査、一連のコースと試験の完了、厳格な倫理規定への準拠など、いくつかの要件を満たす必要があります。認定されると、インスペクターは厳格な CPI 認定プロフェッショナル インスペクター CPI® 資格を使用できます。これにより、専門性と卓越性への取り組みが証明され、専門家としての地位が確立されます。\\n\\nさらに、InterNACHI は、別の組織である InterNACHI スクールを運営しており、住宅検査官向けにさまざまな認定プログラムと継続教育コースを提供しています。最近、InterNACHI スクールは、前述の .edu ドメインを確保し、その権威ある機関としての地位をさらに確固たるものにしました。 InterNACHI 住宅検査員認定プログラムを修了した InterNACHI スクールの学生は、認定証を受け取ることができます。この証明書には、公式のトランスクリプトが記載されており、教育と資格を目に見える形で証明することができます。\\n\\n結論として、.edu ドメインは、厳格な認定を受けた高等教育機関のみに厳密に予約されている排他的なクラブです。\\n',\n",
       "   'role': 'assistant'}],\n",
       " 'tag': 'aixsatoshi/cosmopedia-japanese-100k'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = shuffled_dataset[random.choice(range(shuffled_dataset.num_rows))]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f7ddea-6fc3-4d45-ba88-c9f9c3ac8514",
   "metadata": {},
   "source": [
    "Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "befd7fd0-9085-4073-b343-f27d4ae5ff89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "locchuong\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05ef139b-c314-4d27-9807-f9017ccbdf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1beacae8cc8241f7b94a5ab43f7d48c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a6f21f67eb44fdbe889556c42ee300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/107 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39caf78981a8478f9073032cd10f28ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/107 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e968a513b2624dc68259a50c325a2995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/107 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/locchuong/llama_conversations/commit/a6eeda988285d567b39c21b5bc789ebabc4cbc3a', commit_message='Upload dataset', commit_description='', oid='a6eeda988285d567b39c21b5bc789ebabc4cbc3a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/locchuong/llama_conversations', endpoint='https://huggingface.co', repo_type='dataset', repo_id='locchuong/llama_conversations'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the dataset to Hugging Face Hub\n",
    "dataset_name = \"llama_conversations\"\n",
    "shuffled_dataset.push_to_hub(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60f969a9-91b0-49c1-9750-61e304a417ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c93f5f109147199cdaad6d3452ba9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/398 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3992d12b914672b01dbaf5b4db736a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00003.parquet:   0%|          | 0.00/51.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b26540d4e34f1882e8c8d6f2887bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00003.parquet:   0%|          | 0.00/51.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168978f14d4f4d69bf49c76c9735bf3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00002-of-00003.parquet:   0%|          | 0.00/51.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b541afc106846cda5d54771cc6656fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/319179 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conversations', 'tag'],\n",
      "        num_rows: 319179\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "loaded_dataset = load_dataset(f\"locchuong/{dataset_name}\")\n",
    "print(loaded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee31bf61-cd4e-4465-b3e2-9c8ccdefd779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['conversations', 'tag'],\n",
      "    num_rows: 1021\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "subset = shuffled_dataset.select(range(int(0.0032 * len(shuffled_dataset))))  # Select first 10%\n",
    "\n",
    "# Print the subset\n",
    "print(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "040eb958-df05-42ec-b153-417a86b56c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e772a8dc4c4e3aa698444f71c45c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b885b00089954a439f80e90d703f69d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/locchuong/llama_conversations_1k/commit/4652ad4c32cec4f7d8f1a10caff1e10123cc32ab', commit_message='Upload dataset', commit_description='', oid='4652ad4c32cec4f7d8f1a10caff1e10123cc32ab', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/locchuong/llama_conversations_1k', endpoint='https://huggingface.co', repo_type='dataset', repo_id='locchuong/llama_conversations_1k'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the dataset to Hugging Face Hub\n",
    "dataset_name = \"llama_conversations_1k\"\n",
    "subset.push_to_hub(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae7329e6-53c7-4bcf-a40c-d620da7b47fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa50e8cc958049519f171a940ca854f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/405 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b38e16c837944efb1d252457ca3ab7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fefc6a56e83464e884ba5016cb28f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1021 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conversations', 'tag'],\n",
      "        num_rows: 1021\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "loaded_dataset = load_dataset(f\"locchuong/{dataset_name}\")\n",
    "print(loaded_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070007e-3f72-4ef8-ba2f-af349c6a51d2",
   "metadata": {},
   "source": [
    "### longquan/llm-japanese-dataset-split_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "345732dd-2232-4920-a2e9-be220061f25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 251655\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"longquan/llm-japanese-dataset-split_10\"\n",
    "dataset = load_dataset(repo_id)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88c361df-2de1-473d-a58f-7a81c931a34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Splits: dict_keys(['train'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are helpful assistant'},\n",
       " {'role': 'user', 'content': '入力されたワードを説明してください。'},\n",
       " {'role': 'user', 'content': 'Kumofs'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Kumofs（くもえふえす）とは、2010年1月18日にえとらぼがApache License 2.0のもとオープンソースとして公開した分散型のストレージシステムである。えとらぼは元ミクシィCTOの衛藤バタラが設立したベンチャー企業。'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View available dataset splits\n",
    "print(\"Available Splits:\", dataset.keys())\n",
    "\n",
    "# Load specific split (e.g., 'train') and inspect the first few rows\n",
    "train_data = dataset[\"train\"]\n",
    "sample = train_data[random.choice(range(train_data.num_rows))] #train_data[0]\n",
    "\n",
    "if sample[\"input\"]:\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\" : sample[\"instruction\"]},\n",
    "            {\"role\": \"user\", \"content\" : sample['input']},\n",
    "            {\"role\": \"assistant\", \"content\" : sample['output']},\n",
    "             ]\n",
    "else:\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\" : sample[\"instruction\"]},\n",
    "            {\"role\": \"assistant\", \"content\" : sample['output']},\n",
    "             ]   \n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f00c073d-872c-480b-bfaf-69ce17efbff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 251655/251655 [00:05<00:00, 48679.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop through the source dataset and append data to the empty dataset\n",
    "new_data = {\"conversations\": [], \"tag\": []}  # Initialize dictionary to store new data\n",
    "for row in tqdm(train_data):\n",
    "    # Create messages\n",
    "    if row[\"input\"]:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\" : row[\"instruction\"]},\n",
    "            {\"role\": \"user\", \"content\" : row['input']},\n",
    "            {\"role\": \"assistant\", \"content\" : row['output']},\n",
    "             ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\" : row[\"instruction\"]},\n",
    "            {\"role\": \"assistant\", \"content\" : row['output']},\n",
    "             ]    \n",
    "    new_data[\"conversations\"].append(messages)\n",
    "    new_data[\"tag\"].append(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cdd8b8eb-48fb-4994-8bac-22412f7c59d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'tag'],\n",
       "    num_rows: 251655\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the collected data to the empty dataset\n",
    "formated_dataset = Dataset.from_dict(new_data)\n",
    "formated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f18f7d8-a0f7-4b5a-9d8e-cca8cd99ac05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'You are helpful assistant', 'role': 'system'},\n",
       "  {'content': '入力されたワードを説明してください。', 'role': 'user'},\n",
       "  {'content': 'レスリー・ショウ', 'role': 'user'},\n",
       "  {'content': 'レスリー・モーティマー・ショウ（Leslie Mortimer Shaw, 1848年11月2日 - 1932年3月28日）は、アメリカ合衆国の実業家、弁護士、政治家。1898年から1902年までアイオワ州知事を、1902年から1907年までアメリカ合衆国財務長官を務めた。',\n",
       "   'role': 'assistant'}],\n",
       " 'tag': 'longquan/llm-japanese-dataset-split_10'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = formated_dataset[random.choice(range(formated_dataset.num_rows))]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80c57cd2-f553-4f0f-b274-40d2361bffd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f053dcb3266343a6936f469a5e1aa29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2bb43c65594f478530114c175cbf56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/252 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/locchuong/llama-longquan-llm-japanese-dataset-split_10/commit/b1c9213973c6a1ded8b138085249d744dfefa83e', commit_message='Upload dataset', commit_description='', oid='b1c9213973c6a1ded8b138085249d744dfefa83e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/locchuong/llama-longquan-llm-japanese-dataset-split_10', endpoint='https://huggingface.co', repo_type='dataset', repo_id='locchuong/llama-longquan-llm-japanese-dataset-split_10'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the dataset to Hugging Face Hub\n",
    "dataset_name = \"llama-longquan-llm-japanese-dataset-split_10\"\n",
    "formated_dataset.push_to_hub(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "714d46c7-46cb-4853-a0d7-87f824473803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499a5cca443542b98f286dbe1cd6acf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/391 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87de951543694bebb5bfca89d43f278a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/55.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a5b81a88a74940a40d582450f4acf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/251655 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conversations', 'tag'],\n",
      "        num_rows: 251655\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "loaded_dataset = load_dataset(f\"locchuong/{dataset_name}\")\n",
    "print(loaded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59e12391-ae67-4938-9d08-0cc4341240b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['conversations', 'tag'],\n",
      "    num_rows: 251\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Select subset\n",
    "subset = formated_dataset.select(range(int(0.001 * len(formated_dataset))))  # Select first 10%\n",
    "\n",
    "# Print the subset\n",
    "print(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1382e2f4-7f06-4a9e-b14a-d590a774539b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587b7b2149cf454b992b6e97ae4275d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a3060bd1df4e4bb04d5958cad7d074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/locchuong/llama-longquan-llm-japanese-dataset-split_10_250/commit/c3d1d7ec70eaf4b375a9666689f705f64a62b631', commit_message='Upload dataset', commit_description='', oid='c3d1d7ec70eaf4b375a9666689f705f64a62b631', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/locchuong/llama-longquan-llm-japanese-dataset-split_10_250', endpoint='https://huggingface.co', repo_type='dataset', repo_id='locchuong/llama-longquan-llm-japanese-dataset-split_10_250'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the dataset to Hugging Face Hub\n",
    "dataset_name = \"llama-longquan-llm-japanese-dataset-split_10_250\"\n",
    "subset.push_to_hub(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7abfbd53-25f1-4b7b-985f-1a8d4f35cc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd4f1da24334f34a55d47e64f186ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/379 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "279f52f6aaf2463b988ccd1c1701866c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/53.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c803bb23a447b98c42bbf2b26e755f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/251 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conversations', 'tag'],\n",
      "        num_rows: 251\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "loaded_dataset = load_dataset(f\"locchuong/{dataset_name}\")\n",
    "print(loaded_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a18721f-4846-49a3-b736-d75ffd3ea0ad",
   "metadata": {},
   "source": [
    "Here's an example of creating an empty dataset using the Hugging Face Datasets library, looping through another dataset, and adding data to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73c304b0-72fc-4b2e-8903-372cbe7f9c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 2\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# Create an empty dataset using a pandas DataFrame\n",
    "empty_df = pd.DataFrame(columns=[\"text\", \"label\"])  # Define your schema here\n",
    "empty_dataset = Dataset.from_pandas(empty_df)\n",
    "\n",
    "# Example dataset to iterate over (you can use your own)\n",
    "data = {\n",
    "    \"text\": [\"Hello world\", \"Hugging Face is awesome\", \"I love NLP\"],\n",
    "    \"label\": [0, 1, 1]\n",
    "}\n",
    "source_dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Loop through the source dataset and append data to the empty dataset\n",
    "new_data = {\"text\": [], \"label\": []}  # Initialize dictionary to store new data\n",
    "for row in source_dataset:\n",
    "    # Process row if needed (e.g., filtering, transforming, etc.)\n",
    "    if row[\"label\"] == 1:  # Example condition\n",
    "        new_data[\"text\"].append(row[\"text\"])\n",
    "        new_data[\"label\"].append(row[\"label\"])\n",
    "\n",
    "# Add the collected data to the empty dataset\n",
    "updated_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "# Print the resulting dataset\n",
    "print(updated_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1578d98c-9101-4170-990b-74be6d0d8feb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
