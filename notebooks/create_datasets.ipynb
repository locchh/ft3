{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1350b10-dccb-40e2-8aed-23a8466fd59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU: Tesla P40\n",
      "Import Successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import sys\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "# Check if a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current device index (default is 0 if no other device is specified)\n",
    "    current_device = torch.cuda.current_device()\n",
    "    \n",
    "    # Get the name of the GPU at this device index\n",
    "    gpu_name = torch.cuda.get_device_name(current_device)\n",
    "    print(f\"Current GPU: {gpu_name}\")\n",
    "else:\n",
    "    print(\"No GPU available.\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, DatasetDict, load_dataset, concatenate_datasets\n",
    "from utils.helper import convert_to_llama_format\n",
    "\n",
    "print(\"Import Successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "898e037d-318c-4888-98af-a8a2f14a3d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataset using a pandas DataFrame\n",
    "empty_df = pd.DataFrame(columns=[\"conversations\", \"tag\"])  # Define your schema here\n",
    "empty_dataset = Dataset.from_pandas(empty_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f762bcbf-1798-4134-b739-15b63243739a",
   "metadata": {},
   "source": [
    "### aixsatoshi/cosmopedia-japanese-100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1889baab-20f5-42b8-b3e2-c70b3ec148ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['format', 'japanese', 'english', 'audience', 'seed_data'],\n",
       "        num_rows: 99866\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"aixsatoshi/cosmopedia-japanese-100k\"\n",
    "dataset = load_dataset(repo_id)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ce9c041-4085-4127-b319-f0ba58e1e758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Splits: dict_keys(['train'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a translator, your job is translate English to Japanese'},\n",
       " {'role': 'user', 'content': 'Translate the given content to Japanese'},\n",
       " {'role': 'user',\n",
       "  'content': ' Industrial rubber belts are essential components in various industrial applications due to their ability to transmit power, reduce friction, and provide flexibility in material handling systems. This section delves into two types of industrial rubber belts - special belts and urethane thumb belts - discussing their design, construction materials, applications, advantages, and selection considerations.\\n\\n**Special Belts:**\\n\\nSpecial belts are specifically engineered to transfer power between rotating shafts while operating at high speeds. They typically consist of multiple layers of fabric reinforcement covered by a wear-resistant rubber or polymer layer, forming a robust and durable structure. The primary function of special belts is power transmission; therefore, they often have a higher power density than other conveyor belts.\\n\\n*Construction Materials:*\\n\\nSpecial belts primarily utilize urethane compounds for their top cover due to their exceptional properties, including high tensile strength, excellent resistance to impact, cutting, tearing, and abrasion, and outstanding load-bearing capabilities. Additionally, urethane offers good chemical resistance against oils, greases, and most solvents commonly found in industrial environments.\\n\\n*Applications:*\\n\\nSpecial belts find widespread usage across diverse industry sectors, including automotive manufacturing, food processing, packaging, printing, pharmaceuticals, pulp & paper, and textiles. Specific uses may involve driving rollers in assembly lines, conveying bulk materials like grains, powders, or small parts, or providing tension control in winding operations.\\n\\n*Advantages:*\\n\\n1. High horsepower capacity allows special belts to handle demanding power transmission tasks efficiently.\\n2. Low energy consumption results from reduced slippage and improved efficiency compared to alternative drive mechanisms such as chains or gears.\\n3. Longer service life stems from increased durability and resistance to degradation caused by chemicals, heat, and wear.\\n4. Easier maintenance arises from fewer required replacement parts and less frequent adjustments.\\n5. Reduced noise levels contribute to quieter operation and better working conditions.\\n6. Versatility enables customization based on application-specific criteria, such as size, shape, and surface texture.\\n7. Enhanced safety features minimize risks associated with manual intervention during installation, removal, or repair.\\n8. Environmental compatibility is achieved through recyclable materials and lower emissions compared to traditional drives.\\n\\n*Selection Considerations:*\\n\\nWhen selecting special belts, several factors should be taken into account, including:\\n\\n1. Power transmission requirements (torque and speed)\\n2. Shaft diameter and alignment accuracy\\n3. Operating temperature and environmental conditions\\n4. Load distribution patterns and weight capacities\\n5. Required belt dimensions (length, width, and thickness)\\n6. Installation constraints (space availability, accessibility, etc.)\\n7. Maintenance schedules and resources\\n8. Cost implications (initial investment, ongoing expenses, return on investment)\\n9. Compliance with relevant standards and regulations\\n\\n**Urethane Thumb Belts:**\\n\\nUrethane thumb belts are specialized industrial rubber belts characterized by their unique tooth profile, which resembles human fingers or thumbs. Designed for heavy-duty applications where positive drivetrain engagement is crucial, these belts offer superior traction and durability even under extreme loads and harsh conditions.\\n\\n*Construction Materials:*\\n\\nLike special belts, urethane thumb belts employ urethane compounds for the top cover due to their remarkable physical and mechanical properties. However, the main distinction lies in the base material – usually steel or strong engineering plastics – molded into the desired finger or thumb pattern.\\n\\n*Applications:*\\n\\nUrethane thumb belts serve vital functions in numerous intensive-use settings, particularly those involving large sprockets or pulleys, significant torques, or irregular surfaces. Examples encompass aggregate processing, mining, metallurgy, lumber mills, sugar refineries, and cement plants. Typical roles might entail serving as track belts for bucket elevators, drag chain conveyors, or apron feeders.\\n\\n*Advantages:*\\n\\n1. Exceptional traction ensures reliable engagement with mating components, minimizing slipping and skidding incidents.\\n2. Robust construction tolerates punishing workloads, prolonged duty cycles, and challenging surroundings.\\n3. Improved longevity reduces downtime for replacements and contributes to overall system reliability.\\n4. Efficient energy utilization leads to decreased operational costs and enhanced sustainability credentials.\\n5. Self-cleaning attributes simplify routine upkeep and decrease maintenance intervals.\\n6. Flexible configuration options cater to varying pitch distances, diameters, and cross sections.\\n7. Easy retrofitting permits straightforward substitution for outdated or worn-out conventional V-belts or synchronous drives.'},\n",
       " {'role': 'user',\n",
       "  'content': '産業ゴムベルトは、電力を送信し、摩擦を減らし、材料処理システムの柔軟性を提供する能力により、さまざまな産業用途に不可欠なコンポーネントです。このセクションでは、特別なベルトとウレタンサムベルトの2種類の産業ゴムベルトを掘り下げ、設計、建設資材、アプリケーション、利点、および選択の考慮事項について議論します。\\n\\n**特別ベルト：**\\n\\n特別なベルトは、高速で動作しながら回転するシャフト間で電力を伝達するように特別に設計されています。それらは通常、耐摩耗性のゴムまたはポリマー層で覆われた布の補強材の複数の層で構成され、堅牢で耐久性のある構造を形成します。特別なベルトの主な機能は、送電容量です。したがって、それらはしばしば他のコンベアベルトよりも高い出力密度を持っています。\\n\\n*建設資材：*\\n\\n特別なベルトは、主に、高張力強度、衝撃、切断、引き裂き、摩耗に対する優れた抵抗、顕著な負荷を負担する能力を含む、例外的な特性のために、トップカバーにウレタン化合物を利用します。さらに、ウレタンは、産業環境でよく見られる油、グリース、およびほとんどの溶媒に対する良好な耐薬品性を提供します。\\n\\n*アプリケーション：*\\n\\n特別なベルトは、自動車製造、食品加工、包装、印刷、医薬品、パルプ＆ペーパー、テキスタイルなど、多様な産業部門で広く使用されています。特定の用途には、アセンブリラインでローラーを駆動すること、穀物、粉末、小さな部品などのバルク材料の伝達、または曲がりくねった操作に張力制御を提供する場合があります。\\n\\n*利点：*\\n\\n1。高い馬力容量により、特別なベルトは、要求の厳しい送電タスクを効率的に処理できます。\\n2.低エネルギー消費は、チェーンやギアなどの代替ドライブメカニズムと比較して、滑りの減少と効率の向上に起因します。\\n3.より長いサービス寿命は、耐久性の向上と、化学物質、熱、摩耗によって引き起こされる分解に対する耐性に起因します。\\n4.必要な交換部品が少なく、頻度の低い調整から、メンテナンスが容易になります。\\n5.騒音レベルの低下は、より静かな操作とより良い労働条件に寄与します。\\n6。汎用性により、サイズ、形状、表面テクスチャなどのアプリケーション固有の基準に基づいてカスタマイズが可能になります。\\n7.安全機能の強化は、設置、除去、または修理中の手動介入に関連するリスクを最小限に抑えます。\\n8.環境の互換性は、従来のドライブと比較して、リサイクル可能な材料と排出量の削減によって達成されます。\\n\\n*選択の考慮事項：*\\n\\n特別なベルトを選択するときは、以下を含むいくつかの要因を考慮する必要があります。\\n\\n1.送電要件（トルクと速度）\\n2.シャフトの直径とアライメント精度\\n3.動作温度と環境条件\\n4.分布パターンと重量容量を積み込みます\\n5。必要なベルト寸法（長さ、幅、厚さ）\\n6.インストール制約（スペースの可用性、アクセシビリティなど）\\n7.メンテナンススケジュールとリソース\\n8.コストへの影響（初期投資、継続的な費用、投資収益率）\\n9.関連する基準と規制への準拠\\n\\n**ウレタンサムベルト：**\\n\\nウレタンサムベルトは、人間の指や親指に似た独自の歯のプロファイルを特徴とする特殊な工業ゴムベルトです。ポジティブなドライブトレインエンゲージメントが非常に重要な頑丈なアプリケーション向けに設計されているため、これらのベルトは、極端な負荷や過酷な条件下でも優れた牽引力と耐久性を提供します。\\n\\n*建設資材：*\\n\\n特別なベルトと同様に、ウレタン親指ベルトは、顕著な物理的および機械的特性のために、上部カバーにウレタン化合物を使用します。ただし、主な区別は、基本材料（通常は鋼鉄または強力なエンジニアリングプラスチック）にあり、目的の指または親指パターンに成形されています。\\n\\n*アプリケーション：*\\n\\nウレタンの親指ベルトは、特に大きなスプロケットや滑車、重大なトルク、または不規則な表面を含む多数の集中的な使用設定で重要な機能を提供します。例には、総処理、鉱業、冶金、製材所、砂糖製油所、セメント植物が含まれます。典型的な役割には、バケットエレベーター、ドラッグチェーンコンベア、またはエプロンフィーダーのトラックベルトとして機能する場合があります。\\n\\n*利点：*\\n\\n1.例外的な牽引力は、交配コンポーネントとの信頼できる関与を保証し、滑りやスキッドインシデントを最小限に抑えます。\\n2.堅牢な建設は、罰のワークロード、長期の勤務サイクル、および挑戦的な環境を容認します。\\n3.寿命の改善は、交換のダウンタイムを短縮し、システム全体の信頼性に貢献します。\\n4.効率的なエネルギー利用により、運用コストが削減され、持続可能性の資格が強化されます。5.セルフクリーニング属性は、日常の維持費を簡素化し、メンテナンス間隔を減らします。\\n6.柔軟な構成オプションは、さまざまなピッチ距離、直径、および断面に対応します。\\n7.簡単に改造することで、時代遅れまたは摩耗した従来のVベルトまたは同期ドライブの簡単な代替を許可します。'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View available dataset splits\n",
    "print(\"Available Splits:\", dataset.keys())\n",
    "\n",
    "# Load specific split (e.g., 'train') and inspect the first few rows\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "sample = train_data[random.choice(range(train_data.num_rows))] #train_data[0]\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a translator, your job is translate English to Japanese\"},\n",
    "    {\"role\": \"user\", \"content\" : \"Translate the given content to Japanese\"},\n",
    "    {\"role\": \"user\", \"content\" : sample['english']},\n",
    "    {\"role\": \"user\", \"content\" : sample['japanese']},\n",
    "     ]\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a293a40c-1d44-4b5d-b8bf-52a5dfb3c17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 99866/99866 [00:02<00:00, 40712.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop through the source dataset and append data to the empty dataset\n",
    "new_data = {\"conversations\": [], \"tag\": []}  # Initialize dictionary to store new data\n",
    "for row in tqdm(train_data):\n",
    "    # Create messages\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a translator, your job is translate English to Japanese\"},\n",
    "    {\"role\": \"user\", \"content\" : \"Translate the given content to Japanese\"},\n",
    "    {\"role\": \"user\", \"content\" : sample['english']},\n",
    "    {\"role\": \"user\", \"content\" : sample['japanese']},\n",
    "     ]\n",
    "    \n",
    "    new_data[\"conversations\"].append(messages)\n",
    "    new_data[\"tag\"].append(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84609664-a353-45c1-907c-fa5f2601288f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['conversations', 'tag'],\n",
      "    num_rows: 99866\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Add the collected data to the empty dataset\n",
    "updated_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "# Concatenate them\n",
    "empty_dataset = concatenate_datasets([empty_dataset, updated_dataset])\n",
    "\n",
    "# Print the resulting dataset\n",
    "print(empty_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b5e9d0-5b62-4a33-8957-6383814b4f76",
   "metadata": {},
   "source": [
    "### shi3z/Japanese_wikipedia_conversation_100K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23980b64-2bb5-42b9-b8df-45183abfda02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 110322\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"shi3z/Japanese_wikipedia_conversation_100K\"\n",
    "dataset = load_dataset(repo_id)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7b573fa-1814-4e4f-8ba4-8b16a4ab4828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Splits: dict_keys(['train'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': '紀元前35年って何の時代ですか？'},\n",
       " {'role': 'assistant', 'content': '紀元前35年は紀元前という西暦の前の時代のことを指します。'},\n",
       " {'role': 'user', 'content': '紀元前35年に何か重要な出来事はありましたか？'},\n",
       " {'role': 'assistant',\n",
       "  'content': '具体的な出来事は特に記されていないようです。紀元前35年について詳しく言及された情報は少ないようです。'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View available dataset splits\n",
    "print(\"Available Splits:\", dataset.keys())\n",
    "\n",
    "# Load specific split (e.g., 'train') and inspect the first few rows\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "sample = train_data[random.choice(range(train_data.num_rows))] #train_data[0]\n",
    "\n",
    "messages = convert_to_llama_format(sample['conversations'])\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "645e5f5b-1b8e-465a-8d6e-c0ae32d68ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 110322/110322 [00:05<00:00, 20794.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop through the source dataset and append data to the empty dataset\n",
    "new_data = {\"conversations\": [], \"tag\": []}  # Initialize dictionary to store new data\n",
    "for row in tqdm(train_data):\n",
    "    # Create messages\n",
    "    messages = convert_to_llama_format(row['conversations'])    \n",
    "    new_data[\"conversations\"].append(messages)\n",
    "    new_data[\"tag\"].append(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57578f48-5317-4e27-b014-e093e634bd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['conversations', 'tag'],\n",
      "    num_rows: 210188\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Add the collected data to the empty dataset\n",
    "updated_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "# Concatenate them\n",
    "empty_dataset = concatenate_datasets([empty_dataset, updated_dataset])\n",
    "\n",
    "# Print the resulting dataset\n",
    "print(empty_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9974aec-8a42-4b19-a591-3fb3dab73aff",
   "metadata": {},
   "source": [
    "### FreedomIntelligence/alpaca-gpt4-japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04d7b143-a7aa-4443-be4c-b041df3bcf52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations', 'id'],\n",
       "        num_rows: 49969\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"FreedomIntelligence/alpaca-gpt4-japanese\"\n",
    "dataset = load_dataset(repo_id)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a7567cf-a760-4663-9e87-39d4769b28af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Splits: dict_keys(['train'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': '以下の文字並べ替え問題を解くアルゴリズムを作成してください。\\naalw'},\n",
       " {'role': 'assistant',\n",
       "  'content': '単語の並び替えを解決するアルゴリズムは次のとおりです。\\n\\n1. 単語リストを取得します - これは英語単語リストである場合も、ユーザの文脈に関連する辞書である場合もあります。\\n2. 混同した単語を入力として取得します。\\n3. 混同した単語を文字のリストに変換します。\\n4. 文字のリストをアルファベット順に並べます。\\n5. 単語リストから可能な単語を格納する空のリストを作成します。\\n6. 単語リストの各単語を反復処理するループを開始します。\\n7. 各単語について、単語を文字のリストに変換します。\\n8. 文字のリストをアルファベット順に並べます。\\n9. 混合した単語からアルファベット順に並べた文字のリストを、単語リストの単語からアルファベット順に並べた文字のリストと比較します。\\n10. 並べ替えたリストが同じである場合、文字が一致し、したがって単語が問題の可能な解決策であることを意味します。\\n11. 可能な単語を可能な単語のリストに追加します。\\n12. すべての単語をチェックするまで、単語リストを反復処理を継続します。\\n13. 入力された混乱した単語の解決策となる可能性がある可能性のある単語のリストを返します。\\n\\n混乱した単語「aalw」の結果は「wall」です。'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View available dataset splits\n",
    "print(\"Available Splits:\", dataset.keys())\n",
    "\n",
    "# Load specific split (e.g., 'train') and inspect the first few rows\n",
    "train_data = dataset[\"train\"]\n",
    "sample = train_data[random.choice(range(train_data.num_rows))] #train_data[0]\n",
    "\n",
    "messages = convert_to_llama_format(sample['conversations'])\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb741269-2084-43f0-bdf9-45d39f7b2ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 49969/49969 [00:01<00:00, 42785.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop through the source dataset and append data to the empty dataset\n",
    "new_data = {\"conversations\": [], \"tag\": []}  # Initialize dictionary to store new data\n",
    "for row in tqdm(train_data):\n",
    "    # Create messages\n",
    "    messages = convert_to_llama_format(row['conversations'])    \n",
    "    new_data[\"conversations\"].append(messages)\n",
    "    new_data[\"tag\"].append(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c21f91f-ee47-439c-8fe7-10f66c9657ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['conversations', 'tag'],\n",
      "    num_rows: 260157\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Add the collected data to the empty dataset\n",
    "updated_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "# Concatenate them\n",
    "empty_dataset = concatenate_datasets([empty_dataset, updated_dataset])\n",
    "\n",
    "# Print the resulting dataset\n",
    "print(empty_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e5d7be-129a-4a43-934f-bef7edd2c333",
   "metadata": {},
   "source": [
    "### FreedomIntelligence/evol-instruct-japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec3060c6-7f03-4e51-9601-42504bc76e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations', 'id'],\n",
       "        num_rows: 59022\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"FreedomIntelligence/evol-instruct-japanese\"\n",
    "dataset = load_dataset(repo_id)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cfab4ab-d7d5-4197-9f90-b94b714a22be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Splits: dict_keys(['train'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'C++を使用して、オンラインショッピング体験に関する満足度を測定するためのアンケート質問を作成する方法は？ 買い物体験のさまざまな側面（使用のしやすさ、商品選択、顧客サービスなど）の変数を含めることを検討してください。ユーザーの以前の回答に基づいて質問を調整するために条件文を組み込むこともできますか？\\n'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'オンラインショッピング体験に関する満足度を測定するために、以下のアンケート質問を作成することができます。\\n\\n1. あなたが最近オンラインで買い物をした場合、そのサイトでの買い物体験についてどの程度満足しましたか？\\n\\n2. 商品の品揃えについてどの程度満足しましたか？\\n\\n3. 商品の価格についてどの程度満足しましたか？\\n\\n4. 商品の品質についてどの程度満足しましたか？\\n\\n5. 商品の説明や詳細情報についてどの程度満足しましたか？\\n\\n6. 商品を探しやすさについてどの程度満足しましたか？\\n\\n7. サイトの使用のしやすさについてどの程度満足しましたか？\\n\\n8. 支払いや配送に関する情報についてどの程度満足しましたか？\\n\\n9. 商品の配送時間についてどの程度満足しましたか？\\n\\n10. カスタマーサービスの質についてどの程度満足しましたか？\\n\\n11. サイトのセキュリティについてどの程度安心できましたか？\\n\\n12. あなたは今後もこのサイトで買い物をすると考えていますか？\\n\\n条件文を組み込むこともできます。たとえば、「前回のアンケートで不満を示した場合、今回改善されたかどうか聞きたい場合などには、条件文を使用することができます。例えば、「前回のアンケートでサイトの使用のしやすさについて不満を示しましたが、今回改善されていると感じますか？」といった具合です。'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View available dataset splits\n",
    "print(\"Available Splits:\", dataset.keys())\n",
    "\n",
    "# Load specific split (e.g., 'train') and inspect the first few rows\n",
    "train_data = dataset[\"train\"]\n",
    "sample = train_data[random.choice(range(train_data.num_rows))] #train_data[0]\n",
    "\n",
    "messages = convert_to_llama_format(sample['conversations'])\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1000c618-5c45-4dd6-bcf8-a8b0eb4ad6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 59022/59022 [00:01<00:00, 43791.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop through the source dataset and append data to the empty dataset\n",
    "new_data = {\"conversations\": [], \"tag\": []}  # Initialize dictionary to store new data\n",
    "for row in tqdm(train_data):\n",
    "    # Create messages\n",
    "    messages = convert_to_llama_format(row['conversations'])    \n",
    "    new_data[\"conversations\"].append(messages)\n",
    "    new_data[\"tag\"].append(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d863abb-9837-402d-8aa5-7e358cd15b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['conversations', 'tag'],\n",
      "    num_rows: 319179\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Add the collected data to the empty dataset\n",
    "updated_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "# Concatenate them\n",
    "empty_dataset = concatenate_datasets([empty_dataset, updated_dataset])\n",
    "\n",
    "# Print the resulting dataset\n",
    "print(empty_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e828bad7-0f38-4d41-b5ac-b8e08fc53242",
   "metadata": {},
   "source": [
    "Shuffle and random select sample in new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a03ee3ef-abd8-4dd7-b37f-2c482676cffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'tag'],\n",
       "    num_rows: 319179\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the dataset\n",
    "shuffled_dataset = empty_dataset.shuffle(seed=42)  # Optional seed for reproducibility\n",
    "shuffled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2834889e-5cee-4862-987d-52d5c631f5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'この作品のタイトルは何ですか？', 'role': 'user'},\n",
       "  {'content': '『アルゴノーツ 伝説の冒険者たち』（アルゴノーツ でんせつのぼうけんしゃたち）というタイトルです。',\n",
       "   'role': 'assistant'},\n",
       "  {'content': '放送された年と国はどこですか？', 'role': 'user'},\n",
       "  {'content': 'この作品は2000年5月7日にアメリカ合衆国で放送されました。', 'role': 'assistant'},\n",
       "  {'content': 'この作品はどのような映像化作品ですか？', 'role': 'user'},\n",
       "  {'content': 'この作品はギリシャ神話のアルゴナウタイの冒険譚を映像化したTVミニシリーズです。また、1963年の特撮映画『アルゴ探検隊の大冒険』の完全リメイク版でもあります。',\n",
       "   'role': 'assistant'},\n",
       "  {'content': '監督や主演は誰ですか？', 'role': 'user'},\n",
       "  {'content': '監督はニック・ウィリングで、主演はジェイソン・ロンドン、デニス・ホッパー、ナターシャ・ヘンストリッジなどです。',\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'この作品は日本で公開されましたか？', 'role': 'user'},\n",
       "  {'content': 'いいえ、この作品は日本で劇場公開されませんでした。ただし、2001年5月25日に日活から180分の＜完全版＞のDVDが発売されました。',\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'この作品のキャッチコピーは何ですか？', 'role': 'user'},\n",
       "  {'content': 'パッケージ裏のキャッチコピーは「名作「アルゴ探検隊の大冒険」新世紀リメイクここに登場!!」です。',\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'キャストについて教えてください。', 'role': 'user'},\n",
       "  {'content': 'キャストには日本語吹替も含まれますが、詳細は文章には記載されていません。', 'role': 'assistant'}],\n",
       " 'tag': 'shi3z/Japanese_wikipedia_conversation_100K'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = shuffled_dataset[random.choice(range(shuffled_dataset.num_rows))]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f7ddea-6fc3-4d45-ba88-c9f9c3ac8514",
   "metadata": {},
   "source": [
    "Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "befd7fd0-9085-4073-b343-f27d4ae5ff89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "locchuong\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "05ef139b-c314-4d27-9807-f9017ccbdf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603b7e04a2254d5488bc7af93cd2bf9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbc4832b2074553bd44edbd372ce9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/107 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a81dbd22fe14251a4e8896d42c11919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/107 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600e5d7e880543e29e6461d6d6d64a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/107 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/locchuong/llama_conversations/commit/e17f97ae2a439728eb2a20bea73ba1daa724025c', commit_message='Upload dataset', commit_description='', oid='e17f97ae2a439728eb2a20bea73ba1daa724025c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/locchuong/llama_conversations', endpoint='https://huggingface.co', repo_type='dataset', repo_id='locchuong/llama_conversations'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the dataset to Hugging Face Hub\n",
    "dataset_name = \"llama_conversations\"\n",
    "shuffled_dataset.push_to_hub(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "60f969a9-91b0-49c1-9750-61e304a417ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0139407c1eaf4ac99eed6e15464c092f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/398 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38a86a68f374aeeafdecbc47985fab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00003.parquet:   0%|          | 0.00/52.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a5c2ff83f548719f412f11fcda8b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00003.parquet:   0%|          | 0.00/52.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5db92d542e4f1f85d48099db261582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00002-of-00003.parquet:   0%|          | 0.00/52.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b124a027402b43868fd3a062b3d323c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/319179 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conversations', 'tag'],\n",
      "        num_rows: 319179\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "loaded_dataset = load_dataset(f\"locchuong/{dataset_name}\")\n",
    "print(loaded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ee31bf61-cd4e-4465-b3e2-9c8ccdefd779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['conversations', 'tag'],\n",
      "    num_rows: 1021\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "subset = shuffled_dataset.select(range(int(0.0032 * len(shuffled_dataset))))  # Select first 10%\n",
    "\n",
    "# Print the subset\n",
    "print(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "040eb958-df05-42ec-b153-417a86b56c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02a2ff5fa0e4839a984a287dfa6d35d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68caf13be644416794f3a3afde314596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/locchuong/llama_conversations_1k/commit/909d3003457d4021f49bd610535a8ff3c8cd5f63', commit_message='Upload dataset', commit_description='', oid='909d3003457d4021f49bd610535a8ff3c8cd5f63', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/locchuong/llama_conversations_1k', endpoint='https://huggingface.co', repo_type='dataset', repo_id='locchuong/llama_conversations_1k'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the dataset to Hugging Face Hub\n",
    "dataset_name = \"llama_conversations_1k\"\n",
    "subset.push_to_hub(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ae7329e6-53c7-4bcf-a40c-d620da7b47fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c28b8e7e4be43449625521f340af7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/403 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95cb5ca3234a438aabfec85431b696c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/497k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cdbb9e98ab64af18f5ddccfe4fd33b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1021 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conversations', 'tag'],\n",
      "        num_rows: 1021\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "loaded_dataset = load_dataset(f\"locchuong/{dataset_name}\")\n",
    "print(loaded_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070007e-3f72-4ef8-ba2f-af349c6a51d2",
   "metadata": {},
   "source": [
    "### longquan/llm-japanese-dataset-split_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "345732dd-2232-4920-a2e9-be220061f25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 251655\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"longquan/llm-japanese-dataset-split_10\"\n",
    "dataset = load_dataset(repo_id)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "88c361df-2de1-473d-a58f-7a81c931a34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Splits: dict_keys(['train'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are helpful assistant'},\n",
       " {'role': 'user',\n",
       "  'content': '次の日本語には間違っている部分があります。その部分を直して、正しい日本語の文を出力してください。'},\n",
       " {'role': 'user',\n",
       "  'content': 'ＵＳＥＮ\\u3000がインターネットのブロードバンドをフルに活用して、さまざまな番組コンテンツをを一般のテレビ番組に倣って、提供スポンサー企業からのコマーシャル収入により、ユーザー（利用者）は簡単な手続き（入会金・年会費等諸費完全無料）をおこなうだけでいつでもパソコン上で楽しむことができるオンデマンド放送サービスを２００５年より始めた。'},\n",
       " {'role': 'user',\n",
       "  'content': 'ＵＳＥＮ\\u3000がインターネットのブロードバンドをフルに活用して、さまざまな番組コンテンツを一般のテレビ番組に倣って、提供スポンサー企業からのコマーシャル収入により、ユーザー（利用者）は簡単な手続き（入会金・年会費等諸費完全無料）をおこなうだけでいつでもパソコン上で楽しむことができるオンデマンド放送サービスを２００５年より始めた。'}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View available dataset splits\n",
    "print(\"Available Splits:\", dataset.keys())\n",
    "\n",
    "# Load specific split (e.g., 'train') and inspect the first few rows\n",
    "train_data = dataset[\"train\"]\n",
    "sample = train_data[random.choice(range(train_data.num_rows))] #train_data[0]\n",
    "\n",
    "if sample[\"input\"]:\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\" : sample[\"instruction\"]},\n",
    "            {\"role\": \"user\", \"content\" : sample['input']},\n",
    "            {\"role\": \"user\", \"content\" : sample['output']},\n",
    "             ]\n",
    "else:\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\" : sample[\"instruction\"]},\n",
    "            {\"role\": \"user\", \"content\" : sample['output']},\n",
    "             ]   \n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f00c073d-872c-480b-bfaf-69ce17efbff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 251655/251655 [00:04<00:00, 55747.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop through the source dataset and append data to the empty dataset\n",
    "new_data = {\"conversations\": [], \"tag\": []}  # Initialize dictionary to store new data\n",
    "for row in tqdm(train_data):\n",
    "    # Create messages\n",
    "    if row[\"input\"]:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\" : row[\"instruction\"]},\n",
    "            {\"role\": \"user\", \"content\" : row['input']},\n",
    "            {\"role\": \"user\", \"content\" : row['output']},\n",
    "             ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are helpful assistant\"},\n",
    "            {\"role\": \"user\", \"content\" : row[\"instruction\"]},\n",
    "            {\"role\": \"user\", \"content\" : row['output']},\n",
    "             ]    \n",
    "    new_data[\"conversations\"].append(messages)\n",
    "    new_data[\"tag\"].append(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cdd8b8eb-48fb-4994-8bac-22412f7c59d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations', 'tag'],\n",
       "    num_rows: 251655\n",
       "})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the collected data to the empty dataset\n",
    "formated_dataset = Dataset.from_dict(new_data)\n",
    "formated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0f18f7d8-a0f7-4b5a-9d8e-cca8cd99ac05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': 'You are helpful assistant', 'role': 'system'},\n",
       "  {'content': '入力されたワードを説明してください。', 'role': 'user'},\n",
       "  {'content': '山梨県薬剤師会', 'role': 'user'},\n",
       "  {'content': '一般社団法人山梨県薬剤師会（いっぱんしゃだんほうじんやまなしけんやくざいしかい 英称 Yamanashi Pharmaceutical Association）は、山梨県の薬剤師を会員とする一般社団法人。',\n",
       "   'role': 'user'}],\n",
       " 'tag': 'longquan/llm-japanese-dataset-split_10'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = formated_dataset[random.choice(range(formated_dataset.num_rows))]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "80c57cd2-f553-4f0f-b274-40d2361bffd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ffa7e0266fc4a8b8dd140a320922250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d2d2f00b684605986fa0b6ea42700b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/252 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/locchuong/llama-longquan-llm-japanese-dataset-split_10/commit/4c2412be552e173151cafdbea2b0628c8a8de9c1', commit_message='Upload dataset', commit_description='', oid='4c2412be552e173151cafdbea2b0628c8a8de9c1', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/locchuong/llama-longquan-llm-japanese-dataset-split_10', endpoint='https://huggingface.co', repo_type='dataset', repo_id='locchuong/llama-longquan-llm-japanese-dataset-split_10'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the dataset to Hugging Face Hub\n",
    "dataset_name = \"llama-longquan-llm-japanese-dataset-split_10\"\n",
    "formated_dataset.push_to_hub(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "714d46c7-46cb-4853-a0d7-87f824473803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319e17082a57410d90851a1ea5da925e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/391 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "151a595197b445729826a47691321de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/55.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bbbdb340c84a01bd3e8f8ad900eafd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/251655 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conversations', 'tag'],\n",
      "        num_rows: 251655\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "loaded_dataset = load_dataset(f\"locchuong/{dataset_name}\")\n",
    "print(loaded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "59e12391-ae67-4938-9d08-0cc4341240b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['conversations', 'tag'],\n",
      "    num_rows: 251\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Select subset\n",
    "subset = formated_dataset.select(range(int(0.001 * len(formated_dataset))))  # Select first 10%\n",
    "\n",
    "# Print the subset\n",
    "print(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1382e2f4-7f06-4a9e-b14a-d590a774539b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf84d2ac12c743748d55a000b99e6c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a44eee1d57d4e93ad99073545d0c11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/locchuong/llama-longquan-llm-japanese-dataset-split_10_250/commit/918ea0d1e521f6b46a1ce85b7d34923d8e721817', commit_message='Upload dataset', commit_description='', oid='918ea0d1e521f6b46a1ce85b7d34923d8e721817', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/locchuong/llama-longquan-llm-japanese-dataset-split_10_250', endpoint='https://huggingface.co', repo_type='dataset', repo_id='locchuong/llama-longquan-llm-japanese-dataset-split_10_250'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the dataset to Hugging Face Hub\n",
    "dataset_name = \"llama-longquan-llm-japanese-dataset-split_10_250\"\n",
    "subset.push_to_hub(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7abfbd53-25f1-4b7b-985f-1a8d4f35cc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11fe4ef6a374c378ceccc8196c52bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/379 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0176887bec474182678a1734627c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/53.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7e73772e3f43d6b0a3f22fd0a1ed98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/251 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conversations', 'tag'],\n",
      "        num_rows: 251\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "loaded_dataset = load_dataset(f\"locchuong/{dataset_name}\")\n",
    "print(loaded_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a18721f-4846-49a3-b736-d75ffd3ea0ad",
   "metadata": {},
   "source": [
    "Here's an example of creating an empty dataset using the Hugging Face Datasets library, looping through another dataset, and adding data to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "73c304b0-72fc-4b2e-8903-372cbe7f9c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 2\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# Create an empty dataset using a pandas DataFrame\n",
    "empty_df = pd.DataFrame(columns=[\"text\", \"label\"])  # Define your schema here\n",
    "empty_dataset = Dataset.from_pandas(empty_df)\n",
    "\n",
    "# Example dataset to iterate over (you can use your own)\n",
    "data = {\n",
    "    \"text\": [\"Hello world\", \"Hugging Face is awesome\", \"I love NLP\"],\n",
    "    \"label\": [0, 1, 1]\n",
    "}\n",
    "source_dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Loop through the source dataset and append data to the empty dataset\n",
    "new_data = {\"text\": [], \"label\": []}  # Initialize dictionary to store new data\n",
    "for row in source_dataset:\n",
    "    # Process row if needed (e.g., filtering, transforming, etc.)\n",
    "    if row[\"label\"] == 1:  # Example condition\n",
    "        new_data[\"text\"].append(row[\"text\"])\n",
    "        new_data[\"label\"].append(row[\"label\"])\n",
    "\n",
    "# Add the collected data to the empty dataset\n",
    "updated_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "# Print the resulting dataset\n",
    "print(updated_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
